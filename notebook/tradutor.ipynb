{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tradutor.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHQHiFRjz0rL",
        "colab_type": "text"
      },
      "source": [
        "## Importando os dados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UVmIIaL1uzd",
        "colab_type": "code",
        "outputId": "b9b77322-e0ae-4440-aa6d-c13dc2f1c7c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        }
      },
      "source": [
        "# Run this cell to mount your Google Drive.\n",
        "!jupyter notebook stop 8888\n",
        "!jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There is currently no server running on port 8888\n",
            "Ports currently in use:\n",
            "  - 9000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f562350db235>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'jupyter notebook stop 8888'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/colab/_shell.pyc\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/colab/_system_commands.pyc\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/colab/_system_commands.pyc\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     with temporary_clearer(), _display_stdin_widget(\n\u001b[0;32m--> 181\u001b[0;31m         delay_millis=500) as update_stdin_widget:\n\u001b[0m\u001b[1;32m    182\u001b[0m       \u001b[0;31m# TODO(b/115531839): Ensure that subprocesses are terminated upon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# interrupt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python2.7/contextlib.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/colab/_system_commands.pyc\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0mshell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0mdisplay_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_display_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'delayMillis'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdelay_millis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdisplay_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mecho_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_echo_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/colab/_message.pyc\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/google/colab/_message.pyc\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyVkQw35E2cE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9J6GdPdwqRS",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessando nosso texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA1d_4jcNA-7",
        "colab_type": "text"
      },
      "source": [
        "##Download do texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ail1K60Gw7m9",
        "colab_type": "text"
      },
      "source": [
        "###link do download precisa ser atualizado as vezes, url=http://www.mediafire.com/file/ag4lmha2wqhna72/gu-pt_v1.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0OS9cZDB0Yc",
        "colab_type": "text"
      },
      "source": [
        "### Usando a classificação com o tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlf1OXPUB6-u",
        "colab_type": "code",
        "outputId": "7e3355f7-c503-48d0-b168-3de366a1ccbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install -q tensorflow-gpu==2.0.0-beta1\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 348.9MB 67kB/s \n",
            "\u001b[K     |████████████████████████████████| 501kB 57.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1MB 43.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xfq7ud9vWHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'gu-pt_v1.txt', origin='https://download1641.mediafire.com/dthdkh7da3lg/ag4lmha2wqhna72/gu-pt_v1.txt',\n",
        "    extract=False)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae0gbLuiBpcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "    w = w.rstrip().strip()\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx6HNol-CaqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "   \n",
        "    word_pairs = []\n",
        "    \n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  \n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkNKOQzoZUrc",
        "colab_type": "code",
        "outputId": "60fc1684-900c-4a6b-ce3a-150e219d34b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "gu, pt = create_dataset('gu-pt_v1.txt', 3)\n",
        "\n",
        "print(pt[-1])\n",
        "print(gu[-1])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> entao deus disse que haja luz ! e a luz comecou a existir . <end>\n",
            "<start> ha e gui nhanderuete aipoe i toiko hexakaa he i . ha e ramo hexakaa oiko . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjwX7skkCePU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhvHJAd-ChrU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBaYF9-dCmcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "    # creating cleaned input, output pairs\n",
        "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6zRUTJwC-50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 1500\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset('gu-pt_v1.txt', num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFKBwi8HDDQn",
        "colab_type": "code",
        "outputId": "b2f74de3-b433-4872-bb60-cf15df55f2fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1200, 1200, 300, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT10PgnXv93h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxgFYZbbwHp7",
        "colab_type": "code",
        "outputId": "26f0af38-bf91-4ace-9f07-a0faccb942d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "4 ----> <start>\n",
            "14 ----> ele\n",
            "315 ----> ouviu\n",
            "20 ----> as\n",
            "2676 ----> novidades\n",
            "9 ----> a\n",
            "709 ----> respeito\n",
            "13 ----> do\n",
            "23 ----> seu\n",
            "694 ----> sobrinho\n",
            "3 ----> e\n",
            "387 ----> logo\n",
            "206 ----> saiu\n",
            "648 ----> correndo\n",
            "2 ----> .\n",
            "32 ----> quando\n",
            "624 ----> encontrou\n",
            "26 ----> jaco\n",
            "1 ----> ,\n",
            "102 ----> labao\n",
            "6 ----> o\n",
            "738 ----> abracou\n",
            "1 ----> ,\n",
            "3 ----> e\n",
            "654 ----> beijou\n",
            "1 ----> ,\n",
            "3 ----> e\n",
            "6 ----> o\n",
            "266 ----> levou\n",
            "12 ----> para\n",
            "77 ----> casa\n",
            "2 ----> .\n",
            "26 ----> jaco\n",
            "85 ----> lhe\n",
            "396 ----> contou\n",
            "89 ----> tudo\n",
            "6 ----> o\n",
            "8 ----> que\n",
            "59 ----> havia\n",
            "641 ----> acontecido\n",
            "1 ----> ,\n",
            "5 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "4 ----> <start>\n",
            "783 ----> heindy\n",
            "182 ----> pi\n",
            "21 ----> a\n",
            "31 ----> jaco\n",
            "2269 ----> ovaeague\n",
            "103 ----> labao\n",
            "127 ----> oendu\n",
            "12 ----> vy\n",
            "383 ----> onha\n",
            "33 ----> reve\n",
            "53 ----> oo\n",
            "363 ----> ovaexi\n",
            "2 ----> .\n",
            "198 ----> oikuava\n",
            "7 ----> ,\n",
            "320 ----> oayvu\n",
            "7 ----> ,\n",
            "3 ----> ha\n",
            "1 ----> e\n",
            "263 ----> ngoo\n",
            "15 ----> py\n",
            "131 ----> ogueraa\n",
            "2 ----> .\n",
            "3 ----> ha\n",
            "1 ----> e\n",
            "18 ----> ramo\n",
            "103 ----> labao\n",
            "23 ----> pe\n",
            "31 ----> jaco\n",
            "141 ----> omombe\n",
            "348 ----> upa\n",
            "1094 ----> oguataa\n",
            "36 ----> rupi\n",
            "27 ----> oiko\n",
            "6 ----> va\n",
            "28 ----> ekue\n",
            "2 ----> .\n",
            "5 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1FVHOknwNAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9l1GnSNdwYO",
        "colab_type": "code",
        "outputId": "f5016eeb-5e43-4b07-9bd8-99773d9911b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 95]), TensorShape([64, 94]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3OIwod9xEEK",
        "colab_type": "text"
      },
      "source": [
        "### Write the encoder and decoder model\n",
        "Implement an encoder-decoder model with attention which you can read about in the TensorFlow Neural Machine Translation (seq2seq) tutorial. This example uses a more recent set of APIs. This notebook implements the attention equations from the seq2seq tutorial. The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. The below picture and formulas are an example of attention mechanism from Luong's paper.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZXO2wefwhGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vix-Npicwj0y",
        "colab_type": "code",
        "outputId": "f2196610-c41c-4ac2-f17f-1f747e343dbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 95, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3kemn3pwl-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKCECtVFwomN",
        "colab_type": "code",
        "outputId": "5a221fc7-10dd-46a7-9e40-9b3c34c9f6fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 95, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFWNI9thwrbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsb8ngfFwv6x",
        "colab_type": "code",
        "outputId": "b991bd08-8a71-4ad5-ebc7-db539585e26f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 2877)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi6rHd8TxmKs",
        "colab_type": "text"
      },
      "source": [
        "# Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbkToDK3xoQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg90BH68e-70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDA5p8iixsnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6OIulopxwUj",
        "colab_type": "code",
        "outputId": "376d543a-417c-4fbc-a797-2f7025bb93b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.3222\n",
            "Epoch 1 Loss 1.2992\n",
            "Time taken for 1 epoch 26.6544589996 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 1.2864\n",
            "Epoch 2 Loss 1.2600\n",
            "Time taken for 1 epoch 27.9179799557 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.2406\n",
            "Epoch 3 Loss 1.2244\n",
            "Time taken for 1 epoch 26.6899600029 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.2083\n",
            "Epoch 4 Loss 1.1894\n",
            "Time taken for 1 epoch 27.1147048473 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.1667\n",
            "Epoch 5 Loss 1.1530\n",
            "Time taken for 1 epoch 26.9003460407 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.1339\n",
            "Epoch 6 Loss 1.1206\n",
            "Time taken for 1 epoch 27.3858640194 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.1044\n",
            "Epoch 7 Loss 1.0920\n",
            "Time taken for 1 epoch 26.7317819595 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.0830\n",
            "Epoch 8 Loss 1.0699\n",
            "Time taken for 1 epoch 27.3974659443 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.0511\n",
            "Epoch 9 Loss 1.0494\n",
            "Time taken for 1 epoch 26.8143889904 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.0287\n",
            "Epoch 10 Loss 1.0174\n",
            "Time taken for 1 epoch 27.291383028 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.0012\n",
            "Epoch 11 Loss 0.9940\n",
            "Time taken for 1 epoch 26.7978451252 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.9822\n",
            "Epoch 12 Loss 0.9758\n",
            "Time taken for 1 epoch 27.3001470566 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.9554\n",
            "Epoch 13 Loss 0.9574\n",
            "Time taken for 1 epoch 26.7822380066 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.9490\n",
            "Epoch 14 Loss 0.9353\n",
            "Time taken for 1 epoch 27.2562870979 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.9315\n",
            "Epoch 15 Loss 0.9211\n",
            "Time taken for 1 epoch 26.7891459465 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.8962\n",
            "Epoch 16 Loss 0.9070\n",
            "Time taken for 1 epoch 27.3025171757 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.8857\n",
            "Epoch 17 Loss 0.8915\n",
            "Time taken for 1 epoch 26.7753560543 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.8535\n",
            "Epoch 18 Loss 0.8537\n",
            "Time taken for 1 epoch 27.1924881935 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.8271\n",
            "Epoch 19 Loss 0.8201\n",
            "Time taken for 1 epoch 26.7653408051 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.7988\n",
            "Epoch 20 Loss 0.7946\n",
            "Time taken for 1 epoch 27.2085821629 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSfG9XHkx05n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-mM6J45x3V5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qqa_Agqx5lL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rucHhMiU0_bN",
        "colab_type": "code",
        "outputId": "af632c0e-700f-4c83-ade6-b1eb570e3714",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "translate('No começo Deus criou os céus e a terra.')"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> no comeco deus criou os ceus e a terra . <end>\n",
            "Predicted translation: ha e javi . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAngAAAFoCAYAAADEqYY8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP95ljaLf1kysAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5V9NswaEUBX",
        "colab_type": "text"
      },
      "source": [
        "Portions of this page are reproduced from work created and shared by Google and used according to terms described in the Creative Commons 3.0 Attribution License."
      ]
    }
  ]
}