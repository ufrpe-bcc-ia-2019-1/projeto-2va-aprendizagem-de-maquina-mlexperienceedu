{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tradutor.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA1d_4jcNA-7",
        "colab_type": "text"
      },
      "source": [
        "##Download do texto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfBn1mGAf1Nr",
        "colab_type": "code",
        "outputId": "a5f49e49-f87a-4892-e3dc-ef7c585fc7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "!cp \"/content/drive/My Drive/Projetos/IA/raw_data/gu-pt.txt\" \"gu-pt.txt\"\n",
        "!cp \"/content/drive/My Drive/Projetos/IA/raw_data/xa-pt.txt\" \"xa-pt.txt\"\n",
        "!cp \"/content/drive/My Drive/Projetos/IA/raw_data/ka-pt.txt\" \"ka-pt.txt\"\n",
        "!cp \"/content/drive/My Drive/Projetos/IA/raw_data/tu-pt.txt\" \"tu-pt.txt\"\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0OS9cZDB0Yc",
        "colab_type": "text"
      },
      "source": [
        "### Usando a classificação com o tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlf1OXPUB6-u",
        "colab_type": "code",
        "outputId": "138aaf41-c643-4632-a606-0cc85fae1ed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "!pip install -q tensorflow-gpu==2.0.0-beta1\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 348.9MB 67kB/s \n",
            "\u001b[K     |████████████████████████████████| 501kB 42.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.1MB 32.0MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae0gbLuiBpcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "    w = w.rstrip().strip()\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx6HNol-CaqN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(path, num_examples):\n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "   \n",
        "    word_pairs = []\n",
        "    \n",
        "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n",
        "  \n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkNKOQzoZUrc",
        "colab_type": "code",
        "outputId": "43baf85d-095e-463f-e59d-30fbed47e7e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "gu, pt = create_dataset('gu-pt.txt', 3)\n",
        "\n",
        "print(pt[-1])\n",
        "print(gu[-1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> juda foi pai de peres e de zera , e a mae deles foi tamar . peres foi pai de esrom , que foi pai de arao . <end>\n",
            "<start> juda tamar re ta ya ma perez ha e zera . perez ra y ma esrom , esrom ra y ma arao , <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjwX7skkCePU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhvHJAd-ChrU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBaYF9-dCmcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_dataset(path, num_examples=None):\n",
        "    # creating cleaned input, output pairs\n",
        "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJrScr3a_r-D",
        "colab_type": "text"
      },
      "source": [
        "### Variando o número de exemplos para treino e teste\n",
        "\n",
        "Usaremos a divisão na forma 80-20 onde 80% são para treino e 20% para teste começando com 250 até 1500 (limite da nossa gpu)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6zRUTJwC-50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trial = 1\n",
        "# Try experimenting with the size of that dataset\n",
        "num_examples = 250\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset('gu-pt.txt', num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = max_length(target_tensor), max_length(input_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFKBwi8HDDQn",
        "colab_type": "code",
        "outputId": "59ce597e-b7b7-412c-9d38-533cb47e1e4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200, 200, 50, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YT10PgnXv93h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxgFYZbbwHp7",
        "colab_type": "code",
        "outputId": "76c25924-fc34-4601-bd39-c6de92436bd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "5 ----> <start>\n",
            "79 ----> felizes\n",
            "20 ----> as\n",
            "40 ----> pessoas\n",
            "4 ----> que\n",
            "84 ----> tem\n",
            "483 ----> misericordia\n",
            "52 ----> dos\n",
            "62 ----> outros\n",
            "1 ----> ,\n",
            "22 ----> pois\n",
            "31 ----> deus\n",
            "290 ----> tera\n",
            "483 ----> misericordia\n",
            "339 ----> delas\n",
            "2 ----> .\n",
            "6 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "5 ----> <start>\n",
            "72 ----> ovy\n",
            "17 ----> a\n",
            "45 ----> ete\n",
            "4 ----> ra\n",
            "654 ----> nhomboaxy\n",
            "8 ----> va\n",
            "1 ----> e\n",
            "7 ----> ,\n",
            "15 ----> mba\n",
            "23 ----> eta\n",
            "2 ----> ha\n",
            "1 ----> e\n",
            "11 ----> kuery\n",
            "42 ----> voi\n",
            "655 ----> omboaxya\n",
            "4 ----> ra\n",
            "3 ----> .\n",
            "6 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1FVHOknwNAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index)+1\n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9l1GnSNdwYO",
        "colab_type": "code",
        "outputId": "fa9c0363-201f-4ba6-a678-78334c50ae00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 61]), TensorShape([64, 71]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3OIwod9xEEK",
        "colab_type": "text"
      },
      "source": [
        "### Write the encoder and decoder model\n",
        "Implement an encoder-decoder model with attention which you can read about in the TensorFlow Neural Machine Translation (seq2seq) tutorial. This example uses a more recent set of APIs. This notebook implements the attention equations from the seq2seq tutorial. The following diagram shows that each input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence. The below picture and formulas are an example of attention mechanism from Luong's paper.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZXO2wefwhGE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vix-Npicwj0y",
        "colab_type": "code",
        "outputId": "eaae1d32-e029-4ffc-edf5-fd7184a62436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 61, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3kemn3pwl-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # hidden shape == (batch_size, hidden size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # we are doing this to perform addition to calculate the score\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKCECtVFwomN",
        "colab_type": "code",
        "outputId": "f01a9f65-6532-460a-9879-2bb3530e5e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(10)\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 61, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFWNI9thwrbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bsb8ngfFwv6x",
        "colab_type": "code",
        "outputId": "199289a8-4d29-4db8-89e8-d6a6426d2a80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((64, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 974)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi6rHd8TxmKs",
        "colab_type": "text"
      },
      "source": [
        "# Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbkToDK3xoQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg90BH68e-70",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDA5p8iixsnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOCV8lvV3nzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_epoch_results(part_1, part_2):\n",
        "  \n",
        "  file = open('results.txt', 'a')\n",
        "  file.write(part_1 + \" \" + part_2)\n",
        "  \n",
        "  file.close()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6OIulopxwUj",
        "colab_type": "code",
        "outputId": "a793d25a-27d6-4311-e0bf-30362b869fbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 2000\n",
        "\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                     batch,\n",
        "                                                     batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "  \n",
        "  part_1 = 'Trial {} Epoch {} Loss {:.4f}'.format(trial, epoch + 1,\n",
        "                                      total_loss / steps_per_epoch) \n",
        "  \n",
        "  part_2 = 'Time {} sec\\n'.format(time.time() - start)\n",
        "  \n",
        "  save_epoch_results(part_1, part_2)\n",
        " \n",
        " \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.8064\n",
            "Epoch 1 Loss 2.7148\n",
            "Time taken for 1 epoch 197.36894083 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.3830\n",
            "Epoch 2 Loss 2.4968\n",
            "Time taken for 1 epoch 5.87811589241 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.2710\n",
            "Epoch 3 Loss 2.1943\n",
            "Time taken for 1 epoch 5.43254995346 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.2694\n",
            "Epoch 4 Loss 2.1965\n",
            "Time taken for 1 epoch 5.81456708908 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.2283\n",
            "Epoch 5 Loss 2.1260\n",
            "Time taken for 1 epoch 5.39525794983 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.1221\n",
            "Epoch 6 Loss 2.0443\n",
            "Time taken for 1 epoch 5.83673810959 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.0751\n",
            "Epoch 7 Loss 2.0092\n",
            "Time taken for 1 epoch 5.39643716812 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.0624\n",
            "Epoch 8 Loss 2.0116\n",
            "Time taken for 1 epoch 6.0023651123 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.0431\n",
            "Epoch 9 Loss 1.9855\n",
            "Time taken for 1 epoch 5.36496186256 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.0159\n",
            "Epoch 10 Loss 1.9665\n",
            "Time taken for 1 epoch 5.84647297859 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.0030\n",
            "Epoch 11 Loss 1.9501\n",
            "Time taken for 1 epoch 5.39078783989 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.9859\n",
            "Epoch 12 Loss 1.9289\n",
            "Time taken for 1 epoch 5.87252402306 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.9620\n",
            "Epoch 13 Loss 1.9010\n",
            "Time taken for 1 epoch 5.37448096275 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.9331\n",
            "Epoch 14 Loss 1.8743\n",
            "Time taken for 1 epoch 5.817704916 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.9087\n",
            "Epoch 15 Loss 1.8526\n",
            "Time taken for 1 epoch 5.36618494987 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.8820\n",
            "Epoch 16 Loss 1.8305\n",
            "Time taken for 1 epoch 5.81609487534 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.8578\n",
            "Epoch 17 Loss 1.8057\n",
            "Time taken for 1 epoch 5.3640768528 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.8331\n",
            "Epoch 18 Loss 1.7812\n",
            "Time taken for 1 epoch 5.92255711555 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.8078\n",
            "Epoch 19 Loss 1.7554\n",
            "Time taken for 1 epoch 5.42625117302 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.7819\n",
            "Epoch 20 Loss 1.7290\n",
            "Time taken for 1 epoch 5.84766602516 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.7522\n",
            "Epoch 21 Loss 1.6998\n",
            "Time taken for 1 epoch 5.40269398689 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.7225\n",
            "Epoch 22 Loss 1.6702\n",
            "Time taken for 1 epoch 5.83203315735 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.6896\n",
            "Epoch 23 Loss 1.6382\n",
            "Time taken for 1 epoch 5.39727878571 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.6537\n",
            "Epoch 24 Loss 1.6048\n",
            "Time taken for 1 epoch 5.77968811989 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 1.6179\n",
            "Epoch 25 Loss 1.5725\n",
            "Time taken for 1 epoch 5.41034698486 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.5894\n",
            "Epoch 26 Loss 1.5480\n",
            "Time taken for 1 epoch 5.85443997383 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.5617\n",
            "Epoch 27 Loss 1.5173\n",
            "Time taken for 1 epoch 5.40791893005 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.5288\n",
            "Epoch 28 Loss 1.4851\n",
            "Time taken for 1 epoch 5.86633205414 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 1.4966\n",
            "Epoch 29 Loss 1.4552\n",
            "Time taken for 1 epoch 5.39096093178 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 1.4615\n",
            "Epoch 30 Loss 1.4221\n",
            "Time taken for 1 epoch 5.78105998039 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 1.4309\n",
            "Epoch 31 Loss 1.3953\n",
            "Time taken for 1 epoch 5.34931087494 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 1.3986\n",
            "Epoch 32 Loss 1.3760\n",
            "Time taken for 1 epoch 5.90237402916 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 1.4046\n",
            "Epoch 33 Loss 1.3573\n",
            "Time taken for 1 epoch 5.38787293434 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 1.3677\n",
            "Epoch 34 Loss 1.3250\n",
            "Time taken for 1 epoch 5.85962295532 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 1.3364\n",
            "Epoch 35 Loss 1.3016\n",
            "Time taken for 1 epoch 5.41275191307 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 1.3051\n",
            "Epoch 36 Loss 1.2715\n",
            "Time taken for 1 epoch 5.8247590065 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 1.2770\n",
            "Epoch 37 Loss 1.2417\n",
            "Time taken for 1 epoch 5.32317113876 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 1.2521\n",
            "Epoch 38 Loss 1.2164\n",
            "Time taken for 1 epoch 5.82420396805 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 1.2268\n",
            "Epoch 39 Loss 1.1890\n",
            "Time taken for 1 epoch 5.42591786385 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 1.1984\n",
            "Epoch 40 Loss 1.1645\n",
            "Time taken for 1 epoch 5.89601182938 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 1.1711\n",
            "Epoch 41 Loss 1.1380\n",
            "Time taken for 1 epoch 5.3663828373 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 1.1442\n",
            "Epoch 42 Loss 1.1135\n",
            "Time taken for 1 epoch 5.88559293747 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 1.1177\n",
            "Epoch 43 Loss 1.0882\n",
            "Time taken for 1 epoch 5.35847091675 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 1.0910\n",
            "Epoch 44 Loss 1.0636\n",
            "Time taken for 1 epoch 5.86631202698 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 1.0649\n",
            "Epoch 45 Loss 1.0398\n",
            "Time taken for 1 epoch 5.38053488731 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 1.0430\n",
            "Epoch 46 Loss 1.0174\n",
            "Time taken for 1 epoch 5.85676121712 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 1.0161\n",
            "Epoch 47 Loss 0.9952\n",
            "Time taken for 1 epoch 5.37139391899 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.9966\n",
            "Epoch 48 Loss 0.9766\n",
            "Time taken for 1 epoch 5.88089990616 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.9797\n",
            "Epoch 49 Loss 0.9614\n",
            "Time taken for 1 epoch 5.38211798668 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.9610\n",
            "Epoch 50 Loss 0.9464\n",
            "Time taken for 1 epoch 5.85389590263 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.9334\n",
            "Epoch 51 Loss 0.9205\n",
            "Time taken for 1 epoch 5.42817306519 sec\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.9053\n",
            "Epoch 52 Loss 0.8942\n",
            "Time taken for 1 epoch 5.85969996452 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.8898\n",
            "Epoch 53 Loss 0.8739\n",
            "Time taken for 1 epoch 5.38486385345 sec\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.8658\n",
            "Epoch 54 Loss 0.8528\n",
            "Time taken for 1 epoch 5.86848807335 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.8448\n",
            "Epoch 55 Loss 0.8331\n",
            "Time taken for 1 epoch 5.39334988594 sec\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.8232\n",
            "Epoch 56 Loss 0.8130\n",
            "Time taken for 1 epoch 5.88614010811 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.8040\n",
            "Epoch 57 Loss 0.7952\n",
            "Time taken for 1 epoch 5.39576101303 sec\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.7901\n",
            "Epoch 58 Loss 0.7780\n",
            "Time taken for 1 epoch 5.84942913055 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.7677\n",
            "Epoch 59 Loss 0.7594\n",
            "Time taken for 1 epoch 5.44310498238 sec\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.7554\n",
            "Epoch 60 Loss 0.7462\n",
            "Time taken for 1 epoch 5.84795117378 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.7397\n",
            "Epoch 61 Loss 0.7363\n",
            "Time taken for 1 epoch 5.37373304367 sec\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.7298\n",
            "Epoch 62 Loss 0.7238\n",
            "Time taken for 1 epoch 5.87881994247 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.7415\n",
            "Epoch 63 Loss 0.7228\n",
            "Time taken for 1 epoch 5.35366201401 sec\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.7016\n",
            "Epoch 64 Loss 0.7009\n",
            "Time taken for 1 epoch 5.82617497444 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.6923\n",
            "Epoch 65 Loss 0.6823\n",
            "Time taken for 1 epoch 5.34278392792 sec\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.6691\n",
            "Epoch 66 Loss 0.6628\n",
            "Time taken for 1 epoch 5.82382106781 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.6536\n",
            "Epoch 67 Loss 0.6449\n",
            "Time taken for 1 epoch 5.42033910751 sec\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.6355\n",
            "Epoch 68 Loss 0.6281\n",
            "Time taken for 1 epoch 5.82177114487 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.6168\n",
            "Epoch 69 Loss 0.6098\n",
            "Time taken for 1 epoch 5.38216996193 sec\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.6017\n",
            "Epoch 70 Loss 0.5944\n",
            "Time taken for 1 epoch 5.89990091324 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.5888\n",
            "Epoch 71 Loss 0.5840\n",
            "Time taken for 1 epoch 5.42853999138 sec\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.5805\n",
            "Epoch 72 Loss 0.5752\n",
            "Time taken for 1 epoch 5.96464586258 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.5697\n",
            "Epoch 73 Loss 0.5672\n",
            "Time taken for 1 epoch 5.35979104042 sec\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.5826\n",
            "Epoch 74 Loss 0.5805\n",
            "Time taken for 1 epoch 5.88023710251 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.5459\n",
            "Epoch 75 Loss 0.5606\n",
            "Time taken for 1 epoch 5.34447598457 sec\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.5545\n",
            "Epoch 76 Loss 0.5446\n",
            "Time taken for 1 epoch 5.80167007446 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.5292\n",
            "Epoch 77 Loss 0.5242\n",
            "Time taken for 1 epoch 5.39952111244 sec\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.5087\n",
            "Epoch 78 Loss 0.5047\n",
            "Time taken for 1 epoch 5.86323595047 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.5056\n",
            "Epoch 79 Loss 0.4934\n",
            "Time taken for 1 epoch 5.31974196434 sec\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.4818\n",
            "Epoch 80 Loss 0.4765\n",
            "Time taken for 1 epoch 5.90094089508 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.4686\n",
            "Epoch 81 Loss 0.4619\n",
            "Time taken for 1 epoch 5.39747691154 sec\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.4594\n",
            "Epoch 82 Loss 0.4499\n",
            "Time taken for 1 epoch 5.90449094772 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.4366\n",
            "Epoch 83 Loss 0.4339\n",
            "Time taken for 1 epoch 5.39142799377 sec\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.4278\n",
            "Epoch 84 Loss 0.4212\n",
            "Time taken for 1 epoch 5.92319417 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.4146\n",
            "Epoch 85 Loss 0.4092\n",
            "Time taken for 1 epoch 5.33532786369 sec\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.4089\n",
            "Epoch 86 Loss 0.4015\n",
            "Time taken for 1 epoch 5.88110995293 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.3920\n",
            "Epoch 87 Loss 0.3925\n",
            "Time taken for 1 epoch 5.40618300438 sec\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.4085\n",
            "Epoch 88 Loss 0.3917\n",
            "Time taken for 1 epoch 5.83161592484 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.3839\n",
            "Epoch 89 Loss 0.3759\n",
            "Time taken for 1 epoch 5.37360095978 sec\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.3641\n",
            "Epoch 90 Loss 0.3582\n",
            "Time taken for 1 epoch 5.81983709335 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.3565\n",
            "Epoch 91 Loss 0.3487\n",
            "Time taken for 1 epoch 5.37789583206 sec\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.3408\n",
            "Epoch 92 Loss 0.3341\n",
            "Time taken for 1 epoch 5.79388809204 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.3372\n",
            "Epoch 93 Loss 0.3250\n",
            "Time taken for 1 epoch 5.38268995285 sec\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.3112\n",
            "Epoch 94 Loss 0.3097\n",
            "Time taken for 1 epoch 5.82588911057 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.3115\n",
            "Epoch 95 Loss 0.3033\n",
            "Time taken for 1 epoch 5.41083407402 sec\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.2983\n",
            "Epoch 96 Loss 0.2931\n",
            "Time taken for 1 epoch 5.84738898277 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.2813\n",
            "Epoch 97 Loss 0.2775\n",
            "Time taken for 1 epoch 5.37981104851 sec\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.2706\n",
            "Epoch 98 Loss 0.2656\n",
            "Time taken for 1 epoch 5.88292098045 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.2623\n",
            "Epoch 99 Loss 0.2560\n",
            "Time taken for 1 epoch 5.37397313118 sec\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.2521\n",
            "Epoch 100 Loss 0.2454\n",
            "Time taken for 1 epoch 5.84408998489 sec\n",
            "\n",
            "Epoch 101 Batch 0 Loss 0.2412\n",
            "Epoch 101 Loss 0.2367\n",
            "Time taken for 1 epoch 5.36803889275 sec\n",
            "\n",
            "Epoch 102 Batch 0 Loss 0.2321\n",
            "Epoch 102 Loss 0.2293\n",
            "Time taken for 1 epoch 5.89269518852 sec\n",
            "\n",
            "Epoch 103 Batch 0 Loss 0.2311\n",
            "Epoch 103 Loss 0.2270\n",
            "Time taken for 1 epoch 5.37027001381 sec\n",
            "\n",
            "Epoch 104 Batch 0 Loss 0.2218\n",
            "Epoch 104 Loss 0.2220\n",
            "Time taken for 1 epoch 5.82825398445 sec\n",
            "\n",
            "Epoch 105 Batch 0 Loss 0.2196\n",
            "Epoch 105 Loss 0.2296\n",
            "Time taken for 1 epoch 5.35436296463 sec\n",
            "\n",
            "Epoch 106 Batch 0 Loss 0.2661\n",
            "Epoch 106 Loss 0.2645\n",
            "Time taken for 1 epoch 5.87713313103 sec\n",
            "\n",
            "Epoch 107 Batch 0 Loss 0.2561\n",
            "Epoch 107 Loss 0.2663\n",
            "Time taken for 1 epoch 5.3739168644 sec\n",
            "\n",
            "Epoch 108 Batch 0 Loss 0.3188\n",
            "Epoch 108 Loss 0.2650\n",
            "Time taken for 1 epoch 5.88780713081 sec\n",
            "\n",
            "Epoch 109 Batch 0 Loss 0.2604\n",
            "Epoch 109 Loss 0.2352\n",
            "Time taken for 1 epoch 5.34309005737 sec\n",
            "\n",
            "Epoch 110 Batch 0 Loss 0.2353\n",
            "Epoch 110 Loss 0.2197\n",
            "Time taken for 1 epoch 5.82880496979 sec\n",
            "\n",
            "Epoch 111 Batch 0 Loss 0.2169\n",
            "Epoch 111 Loss 0.2023\n",
            "Time taken for 1 epoch 5.38070702553 sec\n",
            "\n",
            "Epoch 112 Batch 0 Loss 0.2020\n",
            "Epoch 112 Loss 0.1937\n",
            "Time taken for 1 epoch 5.79714107513 sec\n",
            "\n",
            "Epoch 113 Batch 0 Loss 0.1865\n",
            "Epoch 113 Loss 0.1794\n",
            "Time taken for 1 epoch 5.36986088753 sec\n",
            "\n",
            "Epoch 114 Batch 0 Loss 0.1749\n",
            "Epoch 114 Loss 0.1674\n",
            "Time taken for 1 epoch 5.86540603638 sec\n",
            "\n",
            "Epoch 115 Batch 0 Loss 0.1636\n",
            "Epoch 115 Loss 0.1587\n",
            "Time taken for 1 epoch 5.37656903267 sec\n",
            "\n",
            "Epoch 116 Batch 0 Loss 0.1552\n",
            "Epoch 116 Loss 0.1492\n",
            "Time taken for 1 epoch 5.87823605537 sec\n",
            "\n",
            "Epoch 117 Batch 0 Loss 0.1460\n",
            "Epoch 117 Loss 0.1415\n",
            "Time taken for 1 epoch 5.37877392769 sec\n",
            "\n",
            "Epoch 118 Batch 0 Loss 0.1363\n",
            "Epoch 118 Loss 0.1334\n",
            "Time taken for 1 epoch 5.80733013153 sec\n",
            "\n",
            "Epoch 119 Batch 0 Loss 0.1302\n",
            "Epoch 119 Loss 0.1268\n",
            "Time taken for 1 epoch 5.41847395897 sec\n",
            "\n",
            "Epoch 120 Batch 0 Loss 0.1242\n",
            "Epoch 120 Loss 0.1208\n",
            "Time taken for 1 epoch 5.88549804688 sec\n",
            "\n",
            "Epoch 121 Batch 0 Loss 0.1183\n",
            "Epoch 121 Loss 0.1148\n",
            "Time taken for 1 epoch 5.33598995209 sec\n",
            "\n",
            "Epoch 122 Batch 0 Loss 0.1135\n",
            "Epoch 122 Loss 0.1096\n",
            "Time taken for 1 epoch 5.8471531868 sec\n",
            "\n",
            "Epoch 123 Batch 0 Loss 0.1084\n",
            "Epoch 123 Loss 0.1048\n",
            "Time taken for 1 epoch 5.33921098709 sec\n",
            "\n",
            "Epoch 124 Batch 0 Loss 0.1035\n",
            "Epoch 124 Loss 0.1002\n",
            "Time taken for 1 epoch 5.8412129879 sec\n",
            "\n",
            "Epoch 125 Batch 0 Loss 0.0993\n",
            "Epoch 125 Loss 0.0961\n",
            "Time taken for 1 epoch 5.3772919178 sec\n",
            "\n",
            "Epoch 126 Batch 0 Loss 0.0953\n",
            "Epoch 126 Loss 0.0923\n",
            "Time taken for 1 epoch 5.83271503448 sec\n",
            "\n",
            "Epoch 127 Batch 0 Loss 0.0912\n",
            "Epoch 127 Loss 0.0886\n",
            "Time taken for 1 epoch 5.40095901489 sec\n",
            "\n",
            "Epoch 128 Batch 0 Loss 0.0882\n",
            "Epoch 128 Loss 0.0854\n",
            "Time taken for 1 epoch 5.85463881493 sec\n",
            "\n",
            "Epoch 129 Batch 0 Loss 0.0863\n",
            "Epoch 129 Loss 0.0836\n",
            "Time taken for 1 epoch 5.37266588211 sec\n",
            "\n",
            "Epoch 130 Batch 0 Loss 0.0813\n",
            "Epoch 130 Loss 0.0812\n",
            "Time taken for 1 epoch 5.85564088821 sec\n",
            "\n",
            "Epoch 131 Batch 0 Loss 0.0815\n",
            "Epoch 131 Loss 0.0794\n",
            "Time taken for 1 epoch 5.37853693962 sec\n",
            "\n",
            "Epoch 132 Batch 0 Loss 0.0857\n",
            "Epoch 132 Loss 0.0804\n",
            "Time taken for 1 epoch 5.88572192192 sec\n",
            "\n",
            "Epoch 133 Batch 0 Loss 0.0779\n",
            "Epoch 133 Loss 0.0772\n",
            "Time taken for 1 epoch 5.36683011055 sec\n",
            "\n",
            "Epoch 134 Batch 0 Loss 0.0742\n",
            "Epoch 134 Loss 0.0733\n",
            "Time taken for 1 epoch 5.8572909832 sec\n",
            "\n",
            "Epoch 135 Batch 0 Loss 0.0725\n",
            "Epoch 135 Loss 0.0709\n",
            "Time taken for 1 epoch 5.36762309074 sec\n",
            "\n",
            "Epoch 136 Batch 0 Loss 0.0687\n",
            "Epoch 136 Loss 0.0674\n",
            "Time taken for 1 epoch 5.77751016617 sec\n",
            "\n",
            "Epoch 137 Batch 0 Loss 0.0656\n",
            "Epoch 137 Loss 0.0641\n",
            "Time taken for 1 epoch 5.37382888794 sec\n",
            "\n",
            "Epoch 138 Batch 0 Loss 0.0640\n",
            "Epoch 138 Loss 0.0615\n",
            "Time taken for 1 epoch 5.92859506607 sec\n",
            "\n",
            "Epoch 139 Batch 0 Loss 0.0612\n",
            "Epoch 139 Loss 0.0591\n",
            "Time taken for 1 epoch 5.35942196846 sec\n",
            "\n",
            "Epoch 140 Batch 0 Loss 0.0587\n",
            "Epoch 140 Loss 0.0567\n",
            "Time taken for 1 epoch 5.82488203049 sec\n",
            "\n",
            "Epoch 141 Batch 0 Loss 0.0572\n",
            "Epoch 141 Loss 0.0554\n",
            "Time taken for 1 epoch 5.36921310425 sec\n",
            "\n",
            "Epoch 142 Batch 0 Loss 0.0551\n",
            "Epoch 142 Loss 0.0534\n",
            "Time taken for 1 epoch 5.87180685997 sec\n",
            "\n",
            "Epoch 143 Batch 0 Loss 0.0531\n",
            "Epoch 143 Loss 0.0516\n",
            "Time taken for 1 epoch 5.33348679543 sec\n",
            "\n",
            "Epoch 144 Batch 0 Loss 0.0520\n",
            "Epoch 144 Loss 0.0502\n",
            "Time taken for 1 epoch 5.81684207916 sec\n",
            "\n",
            "Epoch 145 Batch 0 Loss 0.0501\n",
            "Epoch 145 Loss 0.0487\n",
            "Time taken for 1 epoch 5.40448498726 sec\n",
            "\n",
            "Epoch 146 Batch 0 Loss 0.0501\n",
            "Epoch 146 Loss 0.0490\n",
            "Time taken for 1 epoch 5.84451794624 sec\n",
            "\n",
            "Epoch 147 Batch 0 Loss 0.0487\n",
            "Epoch 147 Loss 0.0475\n",
            "Time taken for 1 epoch 5.39948797226 sec\n",
            "\n",
            "Epoch 148 Batch 0 Loss 0.0472\n",
            "Epoch 148 Loss 0.0461\n",
            "Time taken for 1 epoch 5.81692790985 sec\n",
            "\n",
            "Epoch 149 Batch 0 Loss 0.0452\n",
            "Epoch 149 Loss 0.0442\n",
            "Time taken for 1 epoch 5.41475701332 sec\n",
            "\n",
            "Epoch 150 Batch 0 Loss 0.0441\n",
            "Epoch 150 Loss 0.0429\n",
            "Time taken for 1 epoch 5.86276102066 sec\n",
            "\n",
            "Epoch 151 Batch 0 Loss 0.0457\n",
            "Epoch 151 Loss 0.0435\n",
            "Time taken for 1 epoch 5.40488386154 sec\n",
            "\n",
            "Epoch 152 Batch 0 Loss 0.0437\n",
            "Epoch 152 Loss 0.0424\n",
            "Time taken for 1 epoch 5.86275792122 sec\n",
            "\n",
            "Epoch 153 Batch 0 Loss 0.0511\n",
            "Epoch 153 Loss 0.0512\n",
            "Time taken for 1 epoch 5.33699703217 sec\n",
            "\n",
            "Epoch 154 Batch 0 Loss 0.0507\n",
            "Epoch 154 Loss 0.0547\n",
            "Time taken for 1 epoch 5.85197901726 sec\n",
            "\n",
            "Epoch 155 Batch 0 Loss 0.0559\n",
            "Epoch 155 Loss 0.0572\n",
            "Time taken for 1 epoch 5.41338515282 sec\n",
            "\n",
            "Epoch 156 Batch 0 Loss 0.0538\n",
            "Epoch 156 Loss 0.0551\n",
            "Time taken for 1 epoch 5.86990118027 sec\n",
            "\n",
            "Epoch 157 Batch 0 Loss 0.0534\n",
            "Epoch 157 Loss 0.0574\n",
            "Time taken for 1 epoch 5.37848091125 sec\n",
            "\n",
            "Epoch 158 Batch 0 Loss 0.0656\n",
            "Epoch 158 Loss 0.0592\n",
            "Time taken for 1 epoch 5.86569905281 sec\n",
            "\n",
            "Epoch 159 Batch 0 Loss 0.0623\n",
            "Epoch 159 Loss 0.0621\n",
            "Time taken for 1 epoch 5.41937279701 sec\n",
            "\n",
            "Epoch 160 Batch 0 Loss 0.0653\n",
            "Epoch 160 Loss 0.0586\n",
            "Time taken for 1 epoch 5.81970596313 sec\n",
            "\n",
            "Epoch 161 Batch 0 Loss 0.0609\n",
            "Epoch 161 Loss 0.0537\n",
            "Time taken for 1 epoch 5.39191603661 sec\n",
            "\n",
            "Epoch 162 Batch 0 Loss 0.0507\n",
            "Epoch 162 Loss 0.0473\n",
            "Time taken for 1 epoch 5.81497597694 sec\n",
            "\n",
            "Epoch 163 Batch 0 Loss 0.0478\n",
            "Epoch 163 Loss 0.0441\n",
            "Time taken for 1 epoch 5.38774991035 sec\n",
            "\n",
            "Epoch 164 Batch 0 Loss 0.0440\n",
            "Epoch 164 Loss 0.0404\n",
            "Time taken for 1 epoch 5.84839200974 sec\n",
            "\n",
            "Epoch 165 Batch 0 Loss 0.0405\n",
            "Epoch 165 Loss 0.0377\n",
            "Time taken for 1 epoch 5.4107978344 sec\n",
            "\n",
            "Epoch 166 Batch 0 Loss 0.0380\n",
            "Epoch 166 Loss 0.0351\n",
            "Time taken for 1 epoch 5.84169387817 sec\n",
            "\n",
            "Epoch 167 Batch 0 Loss 0.0359\n",
            "Epoch 167 Loss 0.0329\n",
            "Time taken for 1 epoch 5.33783817291 sec\n",
            "\n",
            "Epoch 168 Batch 0 Loss 0.0337\n",
            "Epoch 168 Loss 0.0312\n",
            "Time taken for 1 epoch 5.83628296852 sec\n",
            "\n",
            "Epoch 169 Batch 0 Loss 0.0320\n",
            "Epoch 169 Loss 0.0294\n",
            "Time taken for 1 epoch 5.3752720356 sec\n",
            "\n",
            "Epoch 170 Batch 0 Loss 0.0303\n",
            "Epoch 170 Loss 0.0280\n",
            "Time taken for 1 epoch 5.89301586151 sec\n",
            "\n",
            "Epoch 171 Batch 0 Loss 0.0289\n",
            "Epoch 171 Loss 0.0268\n",
            "Time taken for 1 epoch 5.33358001709 sec\n",
            "\n",
            "Epoch 172 Batch 0 Loss 0.0278\n",
            "Epoch 172 Loss 0.0256\n",
            "Time taken for 1 epoch 5.85783100128 sec\n",
            "\n",
            "Epoch 173 Batch 0 Loss 0.0269\n",
            "Epoch 173 Loss 0.0247\n",
            "Time taken for 1 epoch 5.38114500046 sec\n",
            "\n",
            "Epoch 174 Batch 0 Loss 0.0259\n",
            "Epoch 174 Loss 0.0238\n",
            "Time taken for 1 epoch 5.8415119648 sec\n",
            "\n",
            "Epoch 175 Batch 0 Loss 0.0252\n",
            "Epoch 175 Loss 0.0230\n",
            "Time taken for 1 epoch 5.38849616051 sec\n",
            "\n",
            "Epoch 176 Batch 0 Loss 0.0245\n",
            "Epoch 176 Loss 0.0223\n",
            "Time taken for 1 epoch 5.81569695473 sec\n",
            "\n",
            "Epoch 177 Batch 0 Loss 0.0237\n",
            "Epoch 177 Loss 0.0216\n",
            "Time taken for 1 epoch 5.42741799355 sec\n",
            "\n",
            "Epoch 178 Batch 0 Loss 0.0231\n",
            "Epoch 178 Loss 0.0209\n",
            "Time taken for 1 epoch 5.84702992439 sec\n",
            "\n",
            "Epoch 179 Batch 0 Loss 0.0225\n",
            "Epoch 179 Loss 0.0204\n",
            "Time taken for 1 epoch 5.38607120514 sec\n",
            "\n",
            "Epoch 180 Batch 0 Loss 0.0219\n",
            "Epoch 180 Loss 0.0199\n",
            "Time taken for 1 epoch 5.78592896461 sec\n",
            "\n",
            "Epoch 181 Batch 0 Loss 0.0214\n",
            "Epoch 181 Loss 0.0201\n",
            "Time taken for 1 epoch 5.41071105003 sec\n",
            "\n",
            "Epoch 182 Batch 0 Loss 0.0210\n",
            "Epoch 182 Loss 0.0197\n",
            "Time taken for 1 epoch 5.88393998146 sec\n",
            "\n",
            "Epoch 183 Batch 0 Loss 0.0207\n",
            "Epoch 183 Loss 0.0191\n",
            "Time taken for 1 epoch 5.40298700333 sec\n",
            "\n",
            "Epoch 184 Batch 0 Loss 0.0206\n",
            "Epoch 184 Loss 0.0186\n",
            "Time taken for 1 epoch 5.83907008171 sec\n",
            "\n",
            "Epoch 185 Batch 0 Loss 0.0201\n",
            "Epoch 185 Loss 0.0182\n",
            "Time taken for 1 epoch 5.40479803085 sec\n",
            "\n",
            "Epoch 186 Batch 0 Loss 0.0195\n",
            "Epoch 186 Loss 0.0176\n",
            "Time taken for 1 epoch 5.81858587265 sec\n",
            "\n",
            "Epoch 187 Batch 0 Loss 0.0190\n",
            "Epoch 187 Loss 0.0172\n",
            "Time taken for 1 epoch 5.36741518974 sec\n",
            "\n",
            "Epoch 188 Batch 0 Loss 0.0186\n",
            "Epoch 188 Loss 0.0168\n",
            "Time taken for 1 epoch 5.87364888191 sec\n",
            "\n",
            "Epoch 189 Batch 0 Loss 0.0181\n",
            "Epoch 189 Loss 0.0164\n",
            "Time taken for 1 epoch 5.34009695053 sec\n",
            "\n",
            "Epoch 190 Batch 0 Loss 0.0178\n",
            "Epoch 190 Loss 0.0160\n",
            "Time taken for 1 epoch 5.84211087227 sec\n",
            "\n",
            "Epoch 191 Batch 0 Loss 0.0174\n",
            "Epoch 191 Loss 0.0157\n",
            "Time taken for 1 epoch 5.40948390961 sec\n",
            "\n",
            "Epoch 192 Batch 0 Loss 0.0171\n",
            "Epoch 192 Loss 0.0153\n",
            "Time taken for 1 epoch 5.89543890953 sec\n",
            "\n",
            "Epoch 193 Batch 0 Loss 0.0168\n",
            "Epoch 193 Loss 0.0150\n",
            "Time taken for 1 epoch 5.36337900162 sec\n",
            "\n",
            "Epoch 194 Batch 0 Loss 0.0165\n",
            "Epoch 194 Loss 0.0147\n",
            "Time taken for 1 epoch 5.90266108513 sec\n",
            "\n",
            "Epoch 195 Batch 0 Loss 0.0162\n",
            "Epoch 195 Loss 0.0144\n",
            "Time taken for 1 epoch 5.33195900917 sec\n",
            "\n",
            "Epoch 196 Batch 0 Loss 0.0160\n",
            "Epoch 196 Loss 0.0142\n",
            "Time taken for 1 epoch 5.85032701492 sec\n",
            "\n",
            "Epoch 197 Batch 0 Loss 0.0159\n",
            "Epoch 197 Loss 0.0141\n",
            "Time taken for 1 epoch 5.35641694069 sec\n",
            "\n",
            "Epoch 198 Batch 0 Loss 0.0158\n",
            "Epoch 198 Loss 0.0143\n",
            "Time taken for 1 epoch 5.89965796471 sec\n",
            "\n",
            "Epoch 199 Batch 0 Loss 0.0157\n",
            "Epoch 199 Loss 0.0153\n",
            "Time taken for 1 epoch 5.38945603371 sec\n",
            "\n",
            "Epoch 200 Batch 0 Loss 0.0163\n",
            "Epoch 200 Loss 0.0162\n",
            "Time taken for 1 epoch 5.83133411407 sec\n",
            "\n",
            "Epoch 201 Batch 0 Loss 0.0159\n",
            "Epoch 201 Loss 0.0186\n",
            "Time taken for 1 epoch 5.34667992592 sec\n",
            "\n",
            "Epoch 202 Batch 0 Loss 0.0175\n",
            "Epoch 202 Loss 0.0252\n",
            "Time taken for 1 epoch 5.7120552063 sec\n",
            "\n",
            "Epoch 203 Batch 0 Loss 0.0267\n",
            "Epoch 203 Loss 0.0338\n",
            "Time taken for 1 epoch 5.35125923157 sec\n",
            "\n",
            "Epoch 204 Batch 0 Loss 0.0287\n",
            "Epoch 204 Loss 0.0464\n",
            "Time taken for 1 epoch 5.8905069828 sec\n",
            "\n",
            "Epoch 205 Batch 0 Loss 0.0630\n",
            "Epoch 205 Loss 0.0596\n",
            "Time taken for 1 epoch 5.35749602318 sec\n",
            "\n",
            "Epoch 206 Batch 0 Loss 0.0754\n",
            "Epoch 206 Loss 0.0677\n",
            "Time taken for 1 epoch 5.8476190567 sec\n",
            "\n",
            "Epoch 207 Batch 0 Loss 0.0526\n",
            "Epoch 207 Loss 0.0632\n",
            "Time taken for 1 epoch 5.35950398445 sec\n",
            "\n",
            "Epoch 208 Batch 0 Loss 0.0644\n",
            "Epoch 208 Loss 0.0611\n",
            "Time taken for 1 epoch 5.84736013412 sec\n",
            "\n",
            "Epoch 209 Batch 0 Loss 0.0637\n",
            "Epoch 209 Loss 0.0593\n",
            "Time taken for 1 epoch 5.37349891663 sec\n",
            "\n",
            "Epoch 210 Batch 0 Loss 0.0586\n",
            "Epoch 210 Loss 0.0609\n",
            "Time taken for 1 epoch 5.83941102028 sec\n",
            "\n",
            "Epoch 211 Batch 0 Loss 0.0520\n",
            "Epoch 211 Loss 0.0474\n",
            "Time taken for 1 epoch 5.33603215218 sec\n",
            "\n",
            "Epoch 212 Batch 0 Loss 0.0464\n",
            "Epoch 212 Loss 0.0420\n",
            "Time taken for 1 epoch 5.86255812645 sec\n",
            "\n",
            "Epoch 213 Batch 0 Loss 0.0402\n",
            "Epoch 213 Loss 0.0351\n",
            "Time taken for 1 epoch 5.39947509766 sec\n",
            "\n",
            "Epoch 214 Batch 0 Loss 0.0338\n",
            "Epoch 214 Loss 0.0312\n",
            "Time taken for 1 epoch 5.85392403603 sec\n",
            "\n",
            "Epoch 215 Batch 0 Loss 0.0300\n",
            "Epoch 215 Loss 0.0268\n",
            "Time taken for 1 epoch 5.3898229599 sec\n",
            "\n",
            "Epoch 216 Batch 0 Loss 0.0251\n",
            "Epoch 216 Loss 0.0229\n",
            "Time taken for 1 epoch 5.90547299385 sec\n",
            "\n",
            "Epoch 217 Batch 0 Loss 0.0235\n",
            "Epoch 217 Loss 0.0210\n",
            "Time taken for 1 epoch 5.37158489227 sec\n",
            "\n",
            "Epoch 218 Batch 0 Loss 0.0213\n",
            "Epoch 218 Loss 0.0189\n",
            "Time taken for 1 epoch 5.90253996849 sec\n",
            "\n",
            "Epoch 219 Batch 0 Loss 0.0196\n",
            "Epoch 219 Loss 0.0175\n",
            "Time taken for 1 epoch 5.33057904243 sec\n",
            "\n",
            "Epoch 220 Batch 0 Loss 0.0184\n",
            "Epoch 220 Loss 0.0163\n",
            "Time taken for 1 epoch 5.81595778465 sec\n",
            "\n",
            "Epoch 221 Batch 0 Loss 0.0174\n",
            "Epoch 221 Loss 0.0152\n",
            "Time taken for 1 epoch 5.3707818985 sec\n",
            "\n",
            "Epoch 222 Batch 0 Loss 0.0166\n",
            "Epoch 222 Loss 0.0145\n",
            "Time taken for 1 epoch 5.9301109314 sec\n",
            "\n",
            "Epoch 223 Batch 0 Loss 0.0159\n",
            "Epoch 223 Loss 0.0137\n",
            "Time taken for 1 epoch 5.38904905319 sec\n",
            "\n",
            "Epoch 224 Batch 0 Loss 0.0154\n",
            "Epoch 224 Loss 0.0132\n",
            "Time taken for 1 epoch 5.83560109138 sec\n",
            "\n",
            "Epoch 225 Batch 0 Loss 0.0149\n",
            "Epoch 225 Loss 0.0127\n",
            "Time taken for 1 epoch 5.41655898094 sec\n",
            "\n",
            "Epoch 226 Batch 0 Loss 0.0145\n",
            "Epoch 226 Loss 0.0122\n",
            "Time taken for 1 epoch 5.81596398354 sec\n",
            "\n",
            "Epoch 227 Batch 0 Loss 0.0140\n",
            "Epoch 227 Loss 0.0118\n",
            "Time taken for 1 epoch 5.43640708923 sec\n",
            "\n",
            "Epoch 228 Batch 0 Loss 0.0137\n",
            "Epoch 228 Loss 0.0115\n",
            "Time taken for 1 epoch 5.90522408485 sec\n",
            "\n",
            "Epoch 229 Batch 0 Loss 0.0133\n",
            "Epoch 229 Loss 0.0112\n",
            "Time taken for 1 epoch 5.39533805847 sec\n",
            "\n",
            "Epoch 230 Batch 0 Loss 0.0130\n",
            "Epoch 230 Loss 0.0109\n",
            "Time taken for 1 epoch 5.88050699234 sec\n",
            "\n",
            "Epoch 231 Batch 0 Loss 0.0127\n",
            "Epoch 231 Loss 0.0107\n",
            "Time taken for 1 epoch 5.35578393936 sec\n",
            "\n",
            "Epoch 232 Batch 0 Loss 0.0124\n",
            "Epoch 232 Loss 0.0104\n",
            "Time taken for 1 epoch 5.88207292557 sec\n",
            "\n",
            "Epoch 233 Batch 0 Loss 0.0121\n",
            "Epoch 233 Loss 0.0101\n",
            "Time taken for 1 epoch 5.39128899574 sec\n",
            "\n",
            "Epoch 234 Batch 0 Loss 0.0119\n",
            "Epoch 234 Loss 0.0099\n",
            "Time taken for 1 epoch 5.86226010323 sec\n",
            "\n",
            "Epoch 235 Batch 0 Loss 0.0117\n",
            "Epoch 235 Loss 0.0097\n",
            "Time taken for 1 epoch 5.3600628376 sec\n",
            "\n",
            "Epoch 236 Batch 0 Loss 0.0115\n",
            "Epoch 236 Loss 0.0095\n",
            "Time taken for 1 epoch 5.78608107567 sec\n",
            "\n",
            "Epoch 237 Batch 0 Loss 0.0120\n",
            "Epoch 237 Loss 0.0095\n",
            "Time taken for 1 epoch 5.37168216705 sec\n",
            "\n",
            "Epoch 238 Batch 0 Loss 0.0116\n",
            "Epoch 238 Loss 0.0093\n",
            "Time taken for 1 epoch 5.82390213013 sec\n",
            "\n",
            "Epoch 239 Batch 0 Loss 0.0112\n",
            "Epoch 239 Loss 0.0093\n",
            "Time taken for 1 epoch 5.35524106026 sec\n",
            "\n",
            "Epoch 240 Batch 0 Loss 0.0109\n",
            "Epoch 240 Loss 0.0089\n",
            "Time taken for 1 epoch 5.99781894684 sec\n",
            "\n",
            "Epoch 241 Batch 0 Loss 0.0107\n",
            "Epoch 241 Loss 0.0087\n",
            "Time taken for 1 epoch 5.3864068985 sec\n",
            "\n",
            "Epoch 242 Batch 0 Loss 0.0104\n",
            "Epoch 242 Loss 0.0085\n",
            "Time taken for 1 epoch 5.8376660347 sec\n",
            "\n",
            "Epoch 243 Batch 0 Loss 0.0102\n",
            "Epoch 243 Loss 0.0083\n",
            "Time taken for 1 epoch 5.3985619545 sec\n",
            "\n",
            "Epoch 244 Batch 0 Loss 0.0100\n",
            "Epoch 244 Loss 0.0082\n",
            "Time taken for 1 epoch 5.87805509567 sec\n",
            "\n",
            "Epoch 245 Batch 0 Loss 0.0098\n",
            "Epoch 245 Loss 0.0080\n",
            "Time taken for 1 epoch 5.38737106323 sec\n",
            "\n",
            "Epoch 246 Batch 0 Loss 0.0095\n",
            "Epoch 246 Loss 0.0078\n",
            "Time taken for 1 epoch 5.83712220192 sec\n",
            "\n",
            "Epoch 247 Batch 0 Loss 0.0094\n",
            "Epoch 247 Loss 0.0077\n",
            "Time taken for 1 epoch 5.35495781898 sec\n",
            "\n",
            "Epoch 248 Batch 0 Loss 0.0095\n",
            "Epoch 248 Loss 0.0077\n",
            "Time taken for 1 epoch 5.99294495583 sec\n",
            "\n",
            "Epoch 249 Batch 0 Loss 0.0102\n",
            "Epoch 249 Loss 0.0079\n",
            "Time taken for 1 epoch 5.40800094604 sec\n",
            "\n",
            "Epoch 250 Batch 0 Loss 0.0102\n",
            "Epoch 250 Loss 0.0079\n",
            "Time taken for 1 epoch 5.84033894539 sec\n",
            "\n",
            "Epoch 251 Batch 0 Loss 0.0099\n",
            "Epoch 251 Loss 0.0079\n",
            "Time taken for 1 epoch 5.34389305115 sec\n",
            "\n",
            "Epoch 252 Batch 0 Loss 0.0094\n",
            "Epoch 252 Loss 0.0080\n",
            "Time taken for 1 epoch 5.8314909935 sec\n",
            "\n",
            "Epoch 253 Batch 0 Loss 0.0094\n",
            "Epoch 253 Loss 0.0078\n",
            "Time taken for 1 epoch 5.39130401611 sec\n",
            "\n",
            "Epoch 254 Batch 0 Loss 0.0089\n",
            "Epoch 254 Loss 0.0074\n",
            "Time taken for 1 epoch 5.83259415627 sec\n",
            "\n",
            "Epoch 255 Batch 0 Loss 0.0086\n",
            "Epoch 255 Loss 0.0072\n",
            "Time taken for 1 epoch 5.36900615692 sec\n",
            "\n",
            "Epoch 256 Batch 0 Loss 0.0085\n",
            "Epoch 256 Loss 0.0071\n",
            "Time taken for 1 epoch 5.84300398827 sec\n",
            "\n",
            "Epoch 257 Batch 0 Loss 0.0083\n",
            "Epoch 257 Loss 0.0069\n",
            "Time taken for 1 epoch 5.35445213318 sec\n",
            "\n",
            "Epoch 258 Batch 0 Loss 0.0082\n",
            "Epoch 258 Loss 0.0068\n",
            "Time taken for 1 epoch 5.85474491119 sec\n",
            "\n",
            "Epoch 259 Batch 0 Loss 0.0080\n",
            "Epoch 259 Loss 0.0067\n",
            "Time taken for 1 epoch 5.40251207352 sec\n",
            "\n",
            "Epoch 260 Batch 0 Loss 0.0078\n",
            "Epoch 260 Loss 0.0065\n",
            "Time taken for 1 epoch 5.90729403496 sec\n",
            "\n",
            "Epoch 261 Batch 0 Loss 0.0076\n",
            "Epoch 261 Loss 0.0064\n",
            "Time taken for 1 epoch 5.37079000473 sec\n",
            "\n",
            "Epoch 262 Batch 0 Loss 0.0074\n",
            "Epoch 262 Loss 0.0063\n",
            "Time taken for 1 epoch 5.86105489731 sec\n",
            "\n",
            "Epoch 263 Batch 0 Loss 0.0073\n",
            "Epoch 263 Loss 0.0062\n",
            "Time taken for 1 epoch 5.39504098892 sec\n",
            "\n",
            "Epoch 264 Batch 0 Loss 0.0072\n",
            "Epoch 264 Loss 0.0061\n",
            "Time taken for 1 epoch 5.8605890274 sec\n",
            "\n",
            "Epoch 265 Batch 0 Loss 0.0071\n",
            "Epoch 265 Loss 0.0060\n",
            "Time taken for 1 epoch 5.3525519371 sec\n",
            "\n",
            "Epoch 266 Batch 0 Loss 0.0072\n",
            "Epoch 266 Loss 0.0063\n",
            "Time taken for 1 epoch 5.87141513824 sec\n",
            "\n",
            "Epoch 267 Batch 0 Loss 0.0084\n",
            "Epoch 267 Loss 0.0065\n",
            "Time taken for 1 epoch 5.34518694878 sec\n",
            "\n",
            "Epoch 268 Batch 0 Loss 0.0081\n",
            "Epoch 268 Loss 0.0064\n",
            "Time taken for 1 epoch 5.91246509552 sec\n",
            "\n",
            "Epoch 269 Batch 0 Loss 0.0071\n",
            "Epoch 269 Loss 0.0060\n",
            "Time taken for 1 epoch 5.31831908226 sec\n",
            "\n",
            "Epoch 270 Batch 0 Loss 0.0073\n",
            "Epoch 270 Loss 0.0060\n",
            "Time taken for 1 epoch 5.86464905739 sec\n",
            "\n",
            "Epoch 271 Batch 0 Loss 0.0070\n",
            "Epoch 271 Loss 0.0058\n",
            "Time taken for 1 epoch 5.36620616913 sec\n",
            "\n",
            "Epoch 272 Batch 0 Loss 0.0068\n",
            "Epoch 272 Loss 0.0057\n",
            "Time taken for 1 epoch 5.9015378952 sec\n",
            "\n",
            "Epoch 273 Batch 0 Loss 0.0066\n",
            "Epoch 273 Loss 0.0056\n",
            "Time taken for 1 epoch 5.4149119854 sec\n",
            "\n",
            "Epoch 274 Batch 0 Loss 0.0067\n",
            "Epoch 274 Loss 0.0056\n",
            "Time taken for 1 epoch 5.87660002708 sec\n",
            "\n",
            "Epoch 275 Batch 0 Loss 0.0066\n",
            "Epoch 275 Loss 0.0055\n",
            "Time taken for 1 epoch 5.38279080391 sec\n",
            "\n",
            "Epoch 276 Batch 0 Loss 0.0070\n",
            "Epoch 276 Loss 0.0056\n",
            "Time taken for 1 epoch 5.91375207901 sec\n",
            "\n",
            "Epoch 277 Batch 0 Loss 0.0062\n",
            "Epoch 277 Loss 0.0053\n",
            "Time taken for 1 epoch 5.42037081718 sec\n",
            "\n",
            "Epoch 278 Batch 0 Loss 0.0062\n",
            "Epoch 278 Loss 0.0052\n",
            "Time taken for 1 epoch 5.86474895477 sec\n",
            "\n",
            "Epoch 279 Batch 0 Loss 0.0060\n",
            "Epoch 279 Loss 0.0051\n",
            "Time taken for 1 epoch 5.37429404259 sec\n",
            "\n",
            "Epoch 280 Batch 0 Loss 0.0058\n",
            "Epoch 280 Loss 0.0049\n",
            "Time taken for 1 epoch 5.85285902023 sec\n",
            "\n",
            "Epoch 281 Batch 0 Loss 0.0056\n",
            "Epoch 281 Loss 0.0048\n",
            "Time taken for 1 epoch 5.35696411133 sec\n",
            "\n",
            "Epoch 282 Batch 0 Loss 0.0055\n",
            "Epoch 282 Loss 0.0050\n",
            "Time taken for 1 epoch 5.84290099144 sec\n",
            "\n",
            "Epoch 283 Batch 0 Loss 0.0066\n",
            "Epoch 283 Loss 0.0078\n",
            "Time taken for 1 epoch 5.3560731411 sec\n",
            "\n",
            "Epoch 284 Batch 0 Loss 0.0191\n",
            "Epoch 284 Loss 0.0169\n",
            "Time taken for 1 epoch 5.89406895638 sec\n",
            "\n",
            "Epoch 285 Batch 0 Loss 0.0329\n",
            "Epoch 285 Loss 0.0268\n",
            "Time taken for 1 epoch 5.40048408508 sec\n",
            "\n",
            "Epoch 286 Batch 0 Loss 0.0229\n",
            "Epoch 286 Loss 0.0209\n",
            "Time taken for 1 epoch 5.86833500862 sec\n",
            "\n",
            "Epoch 287 Batch 0 Loss 0.0193\n",
            "Epoch 287 Loss 0.0172\n",
            "Time taken for 1 epoch 5.34158182144 sec\n",
            "\n",
            "Epoch 288 Batch 0 Loss 0.0205\n",
            "Epoch 288 Loss 0.0176\n",
            "Time taken for 1 epoch 5.96370601654 sec\n",
            "\n",
            "Epoch 289 Batch 0 Loss 0.0211\n",
            "Epoch 289 Loss 0.0174\n",
            "Time taken for 1 epoch 5.41194295883 sec\n",
            "\n",
            "Epoch 290 Batch 0 Loss 0.0176\n",
            "Epoch 290 Loss 0.0151\n",
            "Time taken for 1 epoch 5.79386091232 sec\n",
            "\n",
            "Epoch 291 Batch 0 Loss 0.0180\n",
            "Epoch 291 Loss 0.0146\n",
            "Time taken for 1 epoch 5.38161182404 sec\n",
            "\n",
            "Epoch 292 Batch 0 Loss 0.0152\n",
            "Epoch 292 Loss 0.0128\n",
            "Time taken for 1 epoch 5.88363313675 sec\n",
            "\n",
            "Epoch 293 Batch 0 Loss 0.0135\n",
            "Epoch 293 Loss 0.0116\n",
            "Time taken for 1 epoch 5.35939002037 sec\n",
            "\n",
            "Epoch 294 Batch 0 Loss 0.0115\n",
            "Epoch 294 Loss 0.0110\n",
            "Time taken for 1 epoch 5.91141700745 sec\n",
            "\n",
            "Epoch 295 Batch 0 Loss 0.0105\n",
            "Epoch 295 Loss 0.0096\n",
            "Time taken for 1 epoch 5.34664201736 sec\n",
            "\n",
            "Epoch 296 Batch 0 Loss 0.0100\n",
            "Epoch 296 Loss 0.0093\n",
            "Time taken for 1 epoch 5.90265512466 sec\n",
            "\n",
            "Epoch 297 Batch 0 Loss 0.0092\n",
            "Epoch 297 Loss 0.0078\n",
            "Time taken for 1 epoch 5.35603404045 sec\n",
            "\n",
            "Epoch 298 Batch 0 Loss 0.0109\n",
            "Epoch 298 Loss 0.0081\n",
            "Time taken for 1 epoch 5.88586306572 sec\n",
            "\n",
            "Epoch 299 Batch 0 Loss 0.0087\n",
            "Epoch 299 Loss 0.0071\n",
            "Time taken for 1 epoch 5.33011007309 sec\n",
            "\n",
            "Epoch 300 Batch 0 Loss 0.0091\n",
            "Epoch 300 Loss 0.0069\n",
            "Time taken for 1 epoch 5.89366006851 sec\n",
            "\n",
            "Epoch 301 Batch 0 Loss 0.0078\n",
            "Epoch 301 Loss 0.0063\n",
            "Time taken for 1 epoch 5.41922092438 sec\n",
            "\n",
            "Epoch 302 Batch 0 Loss 0.0081\n",
            "Epoch 302 Loss 0.0063\n",
            "Time taken for 1 epoch 5.83391189575 sec\n",
            "\n",
            "Epoch 303 Batch 0 Loss 0.0081\n",
            "Epoch 303 Loss 0.0061\n",
            "Time taken for 1 epoch 5.37556290627 sec\n",
            "\n",
            "Epoch 304 Batch 0 Loss 0.0085\n",
            "Epoch 304 Loss 0.0061\n",
            "Time taken for 1 epoch 5.87134408951 sec\n",
            "\n",
            "Epoch 305 Batch 0 Loss 0.0082\n",
            "Epoch 305 Loss 0.0059\n",
            "Time taken for 1 epoch 5.37655901909 sec\n",
            "\n",
            "Epoch 306 Batch 0 Loss 0.0073\n",
            "Epoch 306 Loss 0.0056\n",
            "Time taken for 1 epoch 5.82669496536 sec\n",
            "\n",
            "Epoch 307 Batch 0 Loss 0.0068\n",
            "Epoch 307 Loss 0.0060\n",
            "Time taken for 1 epoch 5.38137698174 sec\n",
            "\n",
            "Epoch 308 Batch 0 Loss 0.0065\n",
            "Epoch 308 Loss 0.0056\n",
            "Time taken for 1 epoch 5.8084731102 sec\n",
            "\n",
            "Epoch 309 Batch 0 Loss 0.0063\n",
            "Epoch 309 Loss 0.0054\n",
            "Time taken for 1 epoch 5.38957905769 sec\n",
            "\n",
            "Epoch 310 Batch 0 Loss 0.0061\n",
            "Epoch 310 Loss 0.0050\n",
            "Time taken for 1 epoch 5.84343600273 sec\n",
            "\n",
            "Epoch 311 Batch 0 Loss 0.0072\n",
            "Epoch 311 Loss 0.0053\n",
            "Time taken for 1 epoch 5.42815804482 sec\n",
            "\n",
            "Epoch 312 Batch 0 Loss 0.0065\n",
            "Epoch 312 Loss 0.0050\n",
            "Time taken for 1 epoch 5.82759690285 sec\n",
            "\n",
            "Epoch 313 Batch 0 Loss 0.0059\n",
            "Epoch 313 Loss 0.0047\n",
            "Time taken for 1 epoch 5.38930416107 sec\n",
            "\n",
            "Epoch 314 Batch 0 Loss 0.0065\n",
            "Epoch 314 Loss 0.0057\n",
            "Time taken for 1 epoch 5.81027388573 sec\n",
            "\n",
            "Epoch 315 Batch 0 Loss 0.0066\n",
            "Epoch 315 Loss 0.0055\n",
            "Time taken for 1 epoch 5.41707181931 sec\n",
            "\n",
            "Epoch 316 Batch 0 Loss 0.0101\n",
            "Epoch 316 Loss 0.0071\n",
            "Time taken for 1 epoch 5.84563994408 sec\n",
            "\n",
            "Epoch 317 Batch 0 Loss 0.0109\n",
            "Epoch 317 Loss 0.0082\n",
            "Time taken for 1 epoch 5.35049200058 sec\n",
            "\n",
            "Epoch 318 Batch 0 Loss 0.0112\n",
            "Epoch 318 Loss 0.0073\n",
            "Time taken for 1 epoch 5.92329287529 sec\n",
            "\n",
            "Epoch 319 Batch 0 Loss 0.0089\n",
            "Epoch 319 Loss 0.0071\n",
            "Time taken for 1 epoch 5.42101407051 sec\n",
            "\n",
            "Epoch 320 Batch 0 Loss 0.0091\n",
            "Epoch 320 Loss 0.0082\n",
            "Time taken for 1 epoch 5.87843608856 sec\n",
            "\n",
            "Epoch 321 Batch 0 Loss 0.0081\n",
            "Epoch 321 Loss 0.0082\n",
            "Time taken for 1 epoch 5.4176261425 sec\n",
            "\n",
            "Epoch 322 Batch 0 Loss 0.0077\n",
            "Epoch 322 Loss 0.0067\n",
            "Time taken for 1 epoch 5.88198304176 sec\n",
            "\n",
            "Epoch 323 Batch 0 Loss 0.0074\n",
            "Epoch 323 Loss 0.0069\n",
            "Time taken for 1 epoch 5.40763807297 sec\n",
            "\n",
            "Epoch 324 Batch 0 Loss 0.0081\n",
            "Epoch 324 Loss 0.0069\n",
            "Time taken for 1 epoch 5.86823606491 sec\n",
            "\n",
            "Epoch 325 Batch 0 Loss 0.0075\n",
            "Epoch 325 Loss 0.0070\n",
            "Time taken for 1 epoch 5.34777402878 sec\n",
            "\n",
            "Epoch 326 Batch 0 Loss 0.0080\n",
            "Epoch 326 Loss 0.0080\n",
            "Time taken for 1 epoch 5.92915391922 sec\n",
            "\n",
            "Epoch 327 Batch 0 Loss 0.0112\n",
            "Epoch 327 Loss 0.0112\n",
            "Time taken for 1 epoch 5.37765884399 sec\n",
            "\n",
            "Epoch 328 Batch 0 Loss 0.0150\n",
            "Epoch 328 Loss 0.0128\n",
            "Time taken for 1 epoch 5.93355107307 sec\n",
            "\n",
            "Epoch 329 Batch 0 Loss 0.0161\n",
            "Epoch 329 Loss 0.0139\n",
            "Time taken for 1 epoch 5.32628393173 sec\n",
            "\n",
            "Epoch 330 Batch 0 Loss 0.0129\n",
            "Epoch 330 Loss 0.0145\n",
            "Time taken for 1 epoch 5.86250305176 sec\n",
            "\n",
            "Epoch 331 Batch 0 Loss 0.0143\n",
            "Epoch 331 Loss 0.0136\n",
            "Time taken for 1 epoch 5.35378694534 sec\n",
            "\n",
            "Epoch 332 Batch 0 Loss 0.0165\n",
            "Epoch 332 Loss 0.0132\n",
            "Time taken for 1 epoch 5.89137411118 sec\n",
            "\n",
            "Epoch 333 Batch 0 Loss 0.0146\n",
            "Epoch 333 Loss 0.0130\n",
            "Time taken for 1 epoch 5.31088709831 sec\n",
            "\n",
            "Epoch 334 Batch 0 Loss 0.0132\n",
            "Epoch 334 Loss 0.0136\n",
            "Time taken for 1 epoch 5.89011907578 sec\n",
            "\n",
            "Epoch 335 Batch 0 Loss 0.0120\n",
            "Epoch 335 Loss 0.0150\n",
            "Time taken for 1 epoch 5.39726305008 sec\n",
            "\n",
            "Epoch 336 Batch 0 Loss 0.0115\n",
            "Epoch 336 Loss 0.0121\n",
            "Time taken for 1 epoch 5.84658408165 sec\n",
            "\n",
            "Epoch 337 Batch 0 Loss 0.0105\n",
            "Epoch 337 Loss 0.0104\n",
            "Time taken for 1 epoch 5.34583902359 sec\n",
            "\n",
            "Epoch 338 Batch 0 Loss 0.0094\n",
            "Epoch 338 Loss 0.0090\n",
            "Time taken for 1 epoch 5.83845901489 sec\n",
            "\n",
            "Epoch 339 Batch 0 Loss 0.0082\n",
            "Epoch 339 Loss 0.0075\n",
            "Time taken for 1 epoch 5.3499879837 sec\n",
            "\n",
            "Epoch 340 Batch 0 Loss 0.0085\n",
            "Epoch 340 Loss 0.0071\n",
            "Time taken for 1 epoch 5.82484006882 sec\n",
            "\n",
            "Epoch 341 Batch 0 Loss 0.0084\n",
            "Epoch 341 Loss 0.0066\n",
            "Time taken for 1 epoch 5.41124296188 sec\n",
            "\n",
            "Epoch 342 Batch 0 Loss 0.0072\n",
            "Epoch 342 Loss 0.0059\n",
            "Time taken for 1 epoch 5.95085287094 sec\n",
            "\n",
            "Epoch 343 Batch 0 Loss 0.0067\n",
            "Epoch 343 Loss 0.0055\n",
            "Time taken for 1 epoch 5.3942091465 sec\n",
            "\n",
            "Epoch 344 Batch 0 Loss 0.0064\n",
            "Epoch 344 Loss 0.0052\n",
            "Time taken for 1 epoch 5.87096810341 sec\n",
            "\n",
            "Epoch 345 Batch 0 Loss 0.0060\n",
            "Epoch 345 Loss 0.0048\n",
            "Time taken for 1 epoch 5.38016986847 sec\n",
            "\n",
            "Epoch 346 Batch 0 Loss 0.0056\n",
            "Epoch 346 Loss 0.0045\n",
            "Time taken for 1 epoch 5.89542913437 sec\n",
            "\n",
            "Epoch 347 Batch 0 Loss 0.0054\n",
            "Epoch 347 Loss 0.0043\n",
            "Time taken for 1 epoch 5.39260697365 sec\n",
            "\n",
            "Epoch 348 Batch 0 Loss 0.0052\n",
            "Epoch 348 Loss 0.0042\n",
            "Time taken for 1 epoch 5.89916300774 sec\n",
            "\n",
            "Epoch 349 Batch 0 Loss 0.0049\n",
            "Epoch 349 Loss 0.0040\n",
            "Time taken for 1 epoch 5.35406780243 sec\n",
            "\n",
            "Epoch 350 Batch 0 Loss 0.0047\n",
            "Epoch 350 Loss 0.0039\n",
            "Time taken for 1 epoch 5.93360996246 sec\n",
            "\n",
            "Epoch 351 Batch 0 Loss 0.0044\n",
            "Epoch 351 Loss 0.0037\n",
            "Time taken for 1 epoch 5.33237600327 sec\n",
            "\n",
            "Epoch 352 Batch 0 Loss 0.0042\n",
            "Epoch 352 Loss 0.0036\n",
            "Time taken for 1 epoch 5.81573796272 sec\n",
            "\n",
            "Epoch 353 Batch 0 Loss 0.0040\n",
            "Epoch 353 Loss 0.0035\n",
            "Time taken for 1 epoch 5.40639400482 sec\n",
            "\n",
            "Epoch 354 Batch 0 Loss 0.0039\n",
            "Epoch 354 Loss 0.0034\n",
            "Time taken for 1 epoch 5.92813301086 sec\n",
            "\n",
            "Epoch 355 Batch 0 Loss 0.0037\n",
            "Epoch 355 Loss 0.0033\n",
            "Time taken for 1 epoch 5.39591312408 sec\n",
            "\n",
            "Epoch 356 Batch 0 Loss 0.0036\n",
            "Epoch 356 Loss 0.0032\n",
            "Time taken for 1 epoch 5.92986607552 sec\n",
            "\n",
            "Epoch 357 Batch 0 Loss 0.0035\n",
            "Epoch 357 Loss 0.0032\n",
            "Time taken for 1 epoch 5.39832997322 sec\n",
            "\n",
            "Epoch 358 Batch 0 Loss 0.0034\n",
            "Epoch 358 Loss 0.0031\n",
            "Time taken for 1 epoch 5.87835812569 sec\n",
            "\n",
            "Epoch 359 Batch 0 Loss 0.0034\n",
            "Epoch 359 Loss 0.0031\n",
            "Time taken for 1 epoch 5.37535309792 sec\n",
            "\n",
            "Epoch 360 Batch 0 Loss 0.0043\n",
            "Epoch 360 Loss 0.0033\n",
            "Time taken for 1 epoch 5.81435894966 sec\n",
            "\n",
            "Epoch 361 Batch 0 Loss 0.0038\n",
            "Epoch 361 Loss 0.0032\n",
            "Time taken for 1 epoch 5.33880400658 sec\n",
            "\n",
            "Epoch 362 Batch 0 Loss 0.0048\n",
            "Epoch 362 Loss 0.0035\n",
            "Time taken for 1 epoch 5.87796020508 sec\n",
            "\n",
            "Epoch 363 Batch 0 Loss 0.0043\n",
            "Epoch 363 Loss 0.0033\n",
            "Time taken for 1 epoch 5.3875169754 sec\n",
            "\n",
            "Epoch 364 Batch 0 Loss 0.0039\n",
            "Epoch 364 Loss 0.0031\n",
            "Time taken for 1 epoch 5.8196439743 sec\n",
            "\n",
            "Epoch 365 Batch 0 Loss 0.0036\n",
            "Epoch 365 Loss 0.0030\n",
            "Time taken for 1 epoch 5.36569404602 sec\n",
            "\n",
            "Epoch 366 Batch 0 Loss 0.0036\n",
            "Epoch 366 Loss 0.0029\n",
            "Time taken for 1 epoch 5.8589360714 sec\n",
            "\n",
            "Epoch 367 Batch 0 Loss 0.0034\n",
            "Epoch 367 Loss 0.0028\n",
            "Time taken for 1 epoch 5.38318610191 sec\n",
            "\n",
            "Epoch 368 Batch 0 Loss 0.0033\n",
            "Epoch 368 Loss 0.0028\n",
            "Time taken for 1 epoch 5.8856048584 sec\n",
            "\n",
            "Epoch 369 Batch 0 Loss 0.0032\n",
            "Epoch 369 Loss 0.0027\n",
            "Time taken for 1 epoch 5.3609650135 sec\n",
            "\n",
            "Epoch 370 Batch 0 Loss 0.0032\n",
            "Epoch 370 Loss 0.0027\n",
            "Time taken for 1 epoch 5.83219003677 sec\n",
            "\n",
            "Epoch 371 Batch 0 Loss 0.0031\n",
            "Epoch 371 Loss 0.0026\n",
            "Time taken for 1 epoch 5.36977696419 sec\n",
            "\n",
            "Epoch 372 Batch 0 Loss 0.0031\n",
            "Epoch 372 Loss 0.0026\n",
            "Time taken for 1 epoch 5.87227892876 sec\n",
            "\n",
            "Epoch 373 Batch 0 Loss 0.0030\n",
            "Epoch 373 Loss 0.0026\n",
            "Time taken for 1 epoch 5.38057518005 sec\n",
            "\n",
            "Epoch 374 Batch 0 Loss 0.0030\n",
            "Epoch 374 Loss 0.0025\n",
            "Time taken for 1 epoch 5.94498109818 sec\n",
            "\n",
            "Epoch 375 Batch 0 Loss 0.0029\n",
            "Epoch 375 Loss 0.0025\n",
            "Time taken for 1 epoch 5.38161301613 sec\n",
            "\n",
            "Epoch 376 Batch 0 Loss 0.0029\n",
            "Epoch 376 Loss 0.0025\n",
            "Time taken for 1 epoch 5.83341193199 sec\n",
            "\n",
            "Epoch 377 Batch 0 Loss 0.0029\n",
            "Epoch 377 Loss 0.0024\n",
            "Time taken for 1 epoch 5.41475200653 sec\n",
            "\n",
            "Epoch 378 Batch 0 Loss 0.0028\n",
            "Epoch 378 Loss 0.0024\n",
            "Time taken for 1 epoch 5.85910415649 sec\n",
            "\n",
            "Epoch 379 Batch 0 Loss 0.0028\n",
            "Epoch 379 Loss 0.0024\n",
            "Time taken for 1 epoch 5.4280591011 sec\n",
            "\n",
            "Epoch 380 Batch 0 Loss 0.0027\n",
            "Epoch 380 Loss 0.0023\n",
            "Time taken for 1 epoch 5.90765500069 sec\n",
            "\n",
            "Epoch 381 Batch 0 Loss 0.0027\n",
            "Epoch 381 Loss 0.0023\n",
            "Time taken for 1 epoch 5.36774897575 sec\n",
            "\n",
            "Epoch 382 Batch 0 Loss 0.0027\n",
            "Epoch 382 Loss 0.0023\n",
            "Time taken for 1 epoch 5.81215381622 sec\n",
            "\n",
            "Epoch 383 Batch 0 Loss 0.0026\n",
            "Epoch 383 Loss 0.0023\n",
            "Time taken for 1 epoch 5.34102797508 sec\n",
            "\n",
            "Epoch 384 Batch 0 Loss 0.0026\n",
            "Epoch 384 Loss 0.0022\n",
            "Time taken for 1 epoch 5.9327070713 sec\n",
            "\n",
            "Epoch 385 Batch 0 Loss 0.0026\n",
            "Epoch 385 Loss 0.0022\n",
            "Time taken for 1 epoch 5.38146305084 sec\n",
            "\n",
            "Epoch 386 Batch 0 Loss 0.0026\n",
            "Epoch 386 Loss 0.0022\n",
            "Time taken for 1 epoch 5.83064007759 sec\n",
            "\n",
            "Epoch 387 Batch 0 Loss 0.0025\n",
            "Epoch 387 Loss 0.0022\n",
            "Time taken for 1 epoch 5.38346910477 sec\n",
            "\n",
            "Epoch 388 Batch 0 Loss 0.0025\n",
            "Epoch 388 Loss 0.0021\n",
            "Time taken for 1 epoch 5.8222990036 sec\n",
            "\n",
            "Epoch 389 Batch 0 Loss 0.0025\n",
            "Epoch 389 Loss 0.0021\n",
            "Time taken for 1 epoch 5.38424897194 sec\n",
            "\n",
            "Epoch 390 Batch 0 Loss 0.0025\n",
            "Epoch 390 Loss 0.0021\n",
            "Time taken for 1 epoch 5.82020902634 sec\n",
            "\n",
            "Epoch 391 Batch 0 Loss 0.0025\n",
            "Epoch 391 Loss 0.0023\n",
            "Time taken for 1 epoch 5.36913776398 sec\n",
            "\n",
            "Epoch 392 Batch 0 Loss 0.0025\n",
            "Epoch 392 Loss 0.0022\n",
            "Time taken for 1 epoch 5.94014286995 sec\n",
            "\n",
            "Epoch 393 Batch 0 Loss 0.0024\n",
            "Epoch 393 Loss 0.0022\n",
            "Time taken for 1 epoch 5.36259388924 sec\n",
            "\n",
            "Epoch 394 Batch 0 Loss 0.0024\n",
            "Epoch 394 Loss 0.0021\n",
            "Time taken for 1 epoch 5.8727619648 sec\n",
            "\n",
            "Epoch 395 Batch 0 Loss 0.0024\n",
            "Epoch 395 Loss 0.0021\n",
            "Time taken for 1 epoch 5.38586497307 sec\n",
            "\n",
            "Epoch 396 Batch 0 Loss 0.0024\n",
            "Epoch 396 Loss 0.0021\n",
            "Time taken for 1 epoch 5.91625404358 sec\n",
            "\n",
            "Epoch 397 Batch 0 Loss 0.0024\n",
            "Epoch 397 Loss 0.0021\n",
            "Time taken for 1 epoch 5.33480787277 sec\n",
            "\n",
            "Epoch 398 Batch 0 Loss 0.0023\n",
            "Epoch 398 Loss 0.0021\n",
            "Time taken for 1 epoch 5.93366599083 sec\n",
            "\n",
            "Epoch 399 Batch 0 Loss 0.0023\n",
            "Epoch 399 Loss 0.0020\n",
            "Time taken for 1 epoch 5.40289402008 sec\n",
            "\n",
            "Epoch 400 Batch 0 Loss 0.0023\n",
            "Epoch 400 Loss 0.0020\n",
            "Time taken for 1 epoch 5.81497001648 sec\n",
            "\n",
            "Epoch 401 Batch 0 Loss 0.0023\n",
            "Epoch 401 Loss 0.0020\n",
            "Time taken for 1 epoch 5.3876490593 sec\n",
            "\n",
            "Epoch 402 Batch 0 Loss 0.0023\n",
            "Epoch 402 Loss 0.0020\n",
            "Time taken for 1 epoch 5.86043906212 sec\n",
            "\n",
            "Epoch 403 Batch 0 Loss 0.0023\n",
            "Epoch 403 Loss 0.0020\n",
            "Time taken for 1 epoch 5.37385702133 sec\n",
            "\n",
            "Epoch 404 Batch 0 Loss 0.0023\n",
            "Epoch 404 Loss 0.0020\n",
            "Time taken for 1 epoch 5.9182870388 sec\n",
            "\n",
            "Epoch 405 Batch 0 Loss 0.0022\n",
            "Epoch 405 Loss 0.0019\n",
            "Time taken for 1 epoch 5.38398098946 sec\n",
            "\n",
            "Epoch 406 Batch 0 Loss 0.0022\n",
            "Epoch 406 Loss 0.0019\n",
            "Time taken for 1 epoch 5.81607890129 sec\n",
            "\n",
            "Epoch 407 Batch 0 Loss 0.0022\n",
            "Epoch 407 Loss 0.0019\n",
            "Time taken for 1 epoch 5.34777903557 sec\n",
            "\n",
            "Epoch 408 Batch 0 Loss 0.0022\n",
            "Epoch 408 Loss 0.0019\n",
            "Time taken for 1 epoch 5.94053697586 sec\n",
            "\n",
            "Epoch 409 Batch 0 Loss 0.0022\n",
            "Epoch 409 Loss 0.0018\n",
            "Time taken for 1 epoch 5.41840600967 sec\n",
            "\n",
            "Epoch 410 Batch 0 Loss 0.0022\n",
            "Epoch 410 Loss 0.0018\n",
            "Time taken for 1 epoch 5.87389421463 sec\n",
            "\n",
            "Epoch 411 Batch 0 Loss 0.0022\n",
            "Epoch 411 Loss 0.0018\n",
            "Time taken for 1 epoch 5.3763821125 sec\n",
            "\n",
            "Epoch 412 Batch 0 Loss 0.0022\n",
            "Epoch 412 Loss 0.0019\n",
            "Time taken for 1 epoch 5.80767416954 sec\n",
            "\n",
            "Epoch 413 Batch 0 Loss 0.0022\n",
            "Epoch 413 Loss 0.0025\n",
            "Time taken for 1 epoch 5.37651896477 sec\n",
            "\n",
            "Epoch 414 Batch 0 Loss 0.0023\n",
            "Epoch 414 Loss 0.0033\n",
            "Time taken for 1 epoch 5.8446791172 sec\n",
            "\n",
            "Epoch 415 Batch 0 Loss 0.0023\n",
            "Epoch 415 Loss 0.0023\n",
            "Time taken for 1 epoch 5.43064308167 sec\n",
            "\n",
            "Epoch 416 Batch 0 Loss 0.0025\n",
            "Epoch 416 Loss 0.0025\n",
            "Time taken for 1 epoch 5.83770895004 sec\n",
            "\n",
            "Epoch 417 Batch 0 Loss 0.0041\n",
            "Epoch 417 Loss 0.0029\n",
            "Time taken for 1 epoch 5.36888599396 sec\n",
            "\n",
            "Epoch 418 Batch 0 Loss 0.0045\n",
            "Epoch 418 Loss 0.0029\n",
            "Time taken for 1 epoch 5.85355782509 sec\n",
            "\n",
            "Epoch 419 Batch 0 Loss 0.0036\n",
            "Epoch 419 Loss 0.0026\n",
            "Time taken for 1 epoch 5.38677000999 sec\n",
            "\n",
            "Epoch 420 Batch 0 Loss 0.0033\n",
            "Epoch 420 Loss 0.0024\n",
            "Time taken for 1 epoch 5.81548500061 sec\n",
            "\n",
            "Epoch 421 Batch 0 Loss 0.0028\n",
            "Epoch 421 Loss 0.0022\n",
            "Time taken for 1 epoch 5.39224600792 sec\n",
            "\n",
            "Epoch 422 Batch 0 Loss 0.0030\n",
            "Epoch 422 Loss 0.0022\n",
            "Time taken for 1 epoch 5.87092685699 sec\n",
            "\n",
            "Epoch 423 Batch 0 Loss 0.0035\n",
            "Epoch 423 Loss 0.0024\n",
            "Time taken for 1 epoch 5.41643810272 sec\n",
            "\n",
            "Epoch 424 Batch 0 Loss 0.0026\n",
            "Epoch 424 Loss 0.0021\n",
            "Time taken for 1 epoch 5.89625597 sec\n",
            "\n",
            "Epoch 425 Batch 0 Loss 0.0027\n",
            "Epoch 425 Loss 0.0021\n",
            "Time taken for 1 epoch 5.41405105591 sec\n",
            "\n",
            "Epoch 426 Batch 0 Loss 0.0026\n",
            "Epoch 426 Loss 0.0020\n",
            "Time taken for 1 epoch 5.88845014572 sec\n",
            "\n",
            "Epoch 427 Batch 0 Loss 0.0025\n",
            "Epoch 427 Loss 0.0020\n",
            "Time taken for 1 epoch 5.34098911285 sec\n",
            "\n",
            "Epoch 428 Batch 0 Loss 0.0024\n",
            "Epoch 428 Loss 0.0019\n",
            "Time taken for 1 epoch 5.7936861515 sec\n",
            "\n",
            "Epoch 429 Batch 0 Loss 0.0023\n",
            "Epoch 429 Loss 0.0018\n",
            "Time taken for 1 epoch 5.34318614006 sec\n",
            "\n",
            "Epoch 430 Batch 0 Loss 0.0022\n",
            "Epoch 430 Loss 0.0017\n",
            "Time taken for 1 epoch 5.91998505592 sec\n",
            "\n",
            "Epoch 431 Batch 0 Loss 0.0022\n",
            "Epoch 431 Loss 0.0017\n",
            "Time taken for 1 epoch 5.40605401993 sec\n",
            "\n",
            "Epoch 432 Batch 0 Loss 0.0022\n",
            "Epoch 432 Loss 0.0017\n",
            "Time taken for 1 epoch 5.83956098557 sec\n",
            "\n",
            "Epoch 433 Batch 0 Loss 0.0021\n",
            "Epoch 433 Loss 0.0016\n",
            "Time taken for 1 epoch 5.3662340641 sec\n",
            "\n",
            "Epoch 434 Batch 0 Loss 0.0021\n",
            "Epoch 434 Loss 0.0016\n",
            "Time taken for 1 epoch 5.8051071167 sec\n",
            "\n",
            "Epoch 435 Batch 0 Loss 0.0021\n",
            "Epoch 435 Loss 0.0016\n",
            "Time taken for 1 epoch 5.37660503387 sec\n",
            "\n",
            "Epoch 436 Batch 0 Loss 0.0021\n",
            "Epoch 436 Loss 0.0015\n",
            "Time taken for 1 epoch 5.85517501831 sec\n",
            "\n",
            "Epoch 437 Batch 0 Loss 0.0020\n",
            "Epoch 437 Loss 0.0015\n",
            "Time taken for 1 epoch 5.31803607941 sec\n",
            "\n",
            "Epoch 438 Batch 0 Loss 0.0020\n",
            "Epoch 438 Loss 0.0015\n",
            "Time taken for 1 epoch 5.81302404404 sec\n",
            "\n",
            "Epoch 439 Batch 0 Loss 0.0020\n",
            "Epoch 439 Loss 0.0015\n",
            "Time taken for 1 epoch 5.38771700859 sec\n",
            "\n",
            "Epoch 440 Batch 0 Loss 0.0020\n",
            "Epoch 440 Loss 0.0015\n",
            "Time taken for 1 epoch 5.82239699364 sec\n",
            "\n",
            "Epoch 441 Batch 0 Loss 0.0020\n",
            "Epoch 441 Loss 0.0016\n",
            "Time taken for 1 epoch 5.36593818665 sec\n",
            "\n",
            "Epoch 442 Batch 0 Loss 0.0020\n",
            "Epoch 442 Loss 0.0014\n",
            "Time taken for 1 epoch 5.84795999527 sec\n",
            "\n",
            "Epoch 443 Batch 0 Loss 0.0020\n",
            "Epoch 443 Loss 0.0014\n",
            "Time taken for 1 epoch 5.35749912262 sec\n",
            "\n",
            "Epoch 444 Batch 0 Loss 0.0020\n",
            "Epoch 444 Loss 0.0014\n",
            "Time taken for 1 epoch 5.85544991493 sec\n",
            "\n",
            "Epoch 445 Batch 0 Loss 0.0020\n",
            "Epoch 445 Loss 0.0014\n",
            "Time taken for 1 epoch 5.39730000496 sec\n",
            "\n",
            "Epoch 446 Batch 0 Loss 0.0020\n",
            "Epoch 446 Loss 0.0014\n",
            "Time taken for 1 epoch 5.83675289154 sec\n",
            "\n",
            "Epoch 447 Batch 0 Loss 0.0019\n",
            "Epoch 447 Loss 0.0013\n",
            "Time taken for 1 epoch 5.38654017448 sec\n",
            "\n",
            "Epoch 448 Batch 0 Loss 0.0019\n",
            "Epoch 448 Loss 0.0013\n",
            "Time taken for 1 epoch 5.88644695282 sec\n",
            "\n",
            "Epoch 449 Batch 0 Loss 0.0019\n",
            "Epoch 449 Loss 0.0013\n",
            "Time taken for 1 epoch 5.33662080765 sec\n",
            "\n",
            "Epoch 450 Batch 0 Loss 0.0019\n",
            "Epoch 450 Loss 0.0013\n",
            "Time taken for 1 epoch 5.8812148571 sec\n",
            "\n",
            "Epoch 451 Batch 0 Loss 0.0019\n",
            "Epoch 451 Loss 0.0013\n",
            "Time taken for 1 epoch 5.39774918556 sec\n",
            "\n",
            "Epoch 452 Batch 0 Loss 0.0019\n",
            "Epoch 452 Loss 0.0013\n",
            "Time taken for 1 epoch 5.95769405365 sec\n",
            "\n",
            "Epoch 453 Batch 0 Loss 0.0019\n",
            "Epoch 453 Loss 0.0013\n",
            "Time taken for 1 epoch 5.38122081757 sec\n",
            "\n",
            "Epoch 454 Batch 0 Loss 0.0019\n",
            "Epoch 454 Loss 0.0013\n",
            "Time taken for 1 epoch 5.89631509781 sec\n",
            "\n",
            "Epoch 455 Batch 0 Loss 0.0019\n",
            "Epoch 455 Loss 0.0013\n",
            "Time taken for 1 epoch 5.38528990746 sec\n",
            "\n",
            "Epoch 456 Batch 0 Loss 0.0019\n",
            "Epoch 456 Loss 0.0013\n",
            "Time taken for 1 epoch 5.82585692406 sec\n",
            "\n",
            "Epoch 457 Batch 0 Loss 0.0019\n",
            "Epoch 457 Loss 0.0012\n",
            "Time taken for 1 epoch 5.37395405769 sec\n",
            "\n",
            "Epoch 458 Batch 0 Loss 0.0019\n",
            "Epoch 458 Loss 0.0012\n",
            "Time taken for 1 epoch 5.87292003632 sec\n",
            "\n",
            "Epoch 459 Batch 0 Loss 0.0018\n",
            "Epoch 459 Loss 0.0012\n",
            "Time taken for 1 epoch 5.36027789116 sec\n",
            "\n",
            "Epoch 460 Batch 0 Loss 0.0018\n",
            "Epoch 460 Loss 0.0012\n",
            "Time taken for 1 epoch 5.94562411308 sec\n",
            "\n",
            "Epoch 461 Batch 0 Loss 0.0018\n",
            "Epoch 461 Loss 0.0012\n",
            "Time taken for 1 epoch 5.40047001839 sec\n",
            "\n",
            "Epoch 462 Batch 0 Loss 0.0018\n",
            "Epoch 462 Loss 0.0012\n",
            "Time taken for 1 epoch 5.8792090416 sec\n",
            "\n",
            "Epoch 463 Batch 0 Loss 0.0018\n",
            "Epoch 463 Loss 0.0012\n",
            "Time taken for 1 epoch 5.37459397316 sec\n",
            "\n",
            "Epoch 464 Batch 0 Loss 0.0018\n",
            "Epoch 464 Loss 0.0012\n",
            "Time taken for 1 epoch 5.8401620388 sec\n",
            "\n",
            "Epoch 465 Batch 0 Loss 0.0018\n",
            "Epoch 465 Loss 0.0012\n",
            "Time taken for 1 epoch 5.38173508644 sec\n",
            "\n",
            "Epoch 466 Batch 0 Loss 0.0018\n",
            "Epoch 466 Loss 0.0012\n",
            "Time taken for 1 epoch 5.90708804131 sec\n",
            "\n",
            "Epoch 467 Batch 0 Loss 0.0018\n",
            "Epoch 467 Loss 0.0012\n",
            "Time taken for 1 epoch 5.44448113441 sec\n",
            "\n",
            "Epoch 468 Batch 0 Loss 0.0018\n",
            "Epoch 468 Loss 0.0012\n",
            "Time taken for 1 epoch 5.802038908 sec\n",
            "\n",
            "Epoch 469 Batch 0 Loss 0.0018\n",
            "Epoch 469 Loss 0.0012\n",
            "Time taken for 1 epoch 5.42671298981 sec\n",
            "\n",
            "Epoch 470 Batch 0 Loss 0.0018\n",
            "Epoch 470 Loss 0.0012\n",
            "Time taken for 1 epoch 5.85929083824 sec\n",
            "\n",
            "Epoch 471 Batch 0 Loss 0.0018\n",
            "Epoch 471 Loss 0.0012\n",
            "Time taken for 1 epoch 5.36606311798 sec\n",
            "\n",
            "Epoch 472 Batch 0 Loss 0.0018\n",
            "Epoch 472 Loss 0.0012\n",
            "Time taken for 1 epoch 5.90320301056 sec\n",
            "\n",
            "Epoch 473 Batch 0 Loss 0.0018\n",
            "Epoch 473 Loss 0.0011\n",
            "Time taken for 1 epoch 5.40829801559 sec\n",
            "\n",
            "Epoch 474 Batch 0 Loss 0.0018\n",
            "Epoch 474 Loss 0.0011\n",
            "Time taken for 1 epoch 5.89376997948 sec\n",
            "\n",
            "Epoch 475 Batch 0 Loss 0.0018\n",
            "Epoch 475 Loss 0.0011\n",
            "Time taken for 1 epoch 5.37853193283 sec\n",
            "\n",
            "Epoch 476 Batch 0 Loss 0.0017\n",
            "Epoch 476 Loss 0.0011\n",
            "Time taken for 1 epoch 5.80253982544 sec\n",
            "\n",
            "Epoch 477 Batch 0 Loss 0.0017\n",
            "Epoch 477 Loss 0.0011\n",
            "Time taken for 1 epoch 5.44848108292 sec\n",
            "\n",
            "Epoch 478 Batch 0 Loss 0.0017\n",
            "Epoch 478 Loss 0.0011\n",
            "Time taken for 1 epoch 5.94988203049 sec\n",
            "\n",
            "Epoch 479 Batch 0 Loss 0.0017\n",
            "Epoch 479 Loss 0.0011\n",
            "Time taken for 1 epoch 5.36832809448 sec\n",
            "\n",
            "Epoch 480 Batch 0 Loss 0.0017\n",
            "Epoch 480 Loss 0.0011\n",
            "Time taken for 1 epoch 5.904337883 sec\n",
            "\n",
            "Epoch 481 Batch 0 Loss 0.0017\n",
            "Epoch 481 Loss 0.0011\n",
            "Time taken for 1 epoch 5.3673889637 sec\n",
            "\n",
            "Epoch 482 Batch 0 Loss 0.0017\n",
            "Epoch 482 Loss 0.0011\n",
            "Time taken for 1 epoch 5.88693404198 sec\n",
            "\n",
            "Epoch 483 Batch 0 Loss 0.0017\n",
            "Epoch 483 Loss 0.0011\n",
            "Time taken for 1 epoch 5.394094944 sec\n",
            "\n",
            "Epoch 484 Batch 0 Loss 0.0017\n",
            "Epoch 484 Loss 0.0011\n",
            "Time taken for 1 epoch 5.88348793983 sec\n",
            "\n",
            "Epoch 485 Batch 0 Loss 0.0017\n",
            "Epoch 485 Loss 0.0011\n",
            "Time taken for 1 epoch 5.38197493553 sec\n",
            "\n",
            "Epoch 486 Batch 0 Loss 0.0017\n",
            "Epoch 486 Loss 0.0011\n",
            "Time taken for 1 epoch 5.94627809525 sec\n",
            "\n",
            "Epoch 487 Batch 0 Loss 0.0017\n",
            "Epoch 487 Loss 0.0011\n",
            "Time taken for 1 epoch 5.38438200951 sec\n",
            "\n",
            "Epoch 488 Batch 0 Loss 0.0017\n",
            "Epoch 488 Loss 0.0011\n",
            "Time taken for 1 epoch 5.86686897278 sec\n",
            "\n",
            "Epoch 489 Batch 0 Loss 0.0017\n",
            "Epoch 489 Loss 0.0011\n",
            "Time taken for 1 epoch 5.35963511467 sec\n",
            "\n",
            "Epoch 490 Batch 0 Loss 0.0017\n",
            "Epoch 490 Loss 0.0011\n",
            "Time taken for 1 epoch 5.86991691589 sec\n",
            "\n",
            "Epoch 491 Batch 0 Loss 0.0017\n",
            "Epoch 491 Loss 0.0011\n",
            "Time taken for 1 epoch 5.38783693314 sec\n",
            "\n",
            "Epoch 492 Batch 0 Loss 0.0017\n",
            "Epoch 492 Loss 0.0011\n",
            "Time taken for 1 epoch 5.77874898911 sec\n",
            "\n",
            "Epoch 493 Batch 0 Loss 0.0017\n",
            "Epoch 493 Loss 0.0011\n",
            "Time taken for 1 epoch 5.38543891907 sec\n",
            "\n",
            "Epoch 494 Batch 0 Loss 0.0017\n",
            "Epoch 494 Loss 0.0010\n",
            "Time taken for 1 epoch 5.94654989243 sec\n",
            "\n",
            "Epoch 495 Batch 0 Loss 0.0017\n",
            "Epoch 495 Loss 0.0010\n",
            "Time taken for 1 epoch 5.41824197769 sec\n",
            "\n",
            "Epoch 496 Batch 0 Loss 0.0017\n",
            "Epoch 496 Loss 0.0010\n",
            "Time taken for 1 epoch 5.84884691238 sec\n",
            "\n",
            "Epoch 497 Batch 0 Loss 0.0017\n",
            "Epoch 497 Loss 0.0010\n",
            "Time taken for 1 epoch 5.39540100098 sec\n",
            "\n",
            "Epoch 498 Batch 0 Loss 0.0016\n",
            "Epoch 498 Loss 0.0010\n",
            "Time taken for 1 epoch 5.85507392883 sec\n",
            "\n",
            "Epoch 499 Batch 0 Loss 0.0016\n",
            "Epoch 499 Loss 0.0010\n",
            "Time taken for 1 epoch 5.37415003777 sec\n",
            "\n",
            "Epoch 500 Batch 0 Loss 0.0016\n",
            "Epoch 500 Loss 0.0010\n",
            "Time taken for 1 epoch 5.86276197433 sec\n",
            "\n",
            "Epoch 501 Batch 0 Loss 0.0016\n",
            "Epoch 501 Loss 0.0010\n",
            "Time taken for 1 epoch 5.37545681 sec\n",
            "\n",
            "Epoch 502 Batch 0 Loss 0.0016\n",
            "Epoch 502 Loss 0.0010\n",
            "Time taken for 1 epoch 5.8286960125 sec\n",
            "\n",
            "Epoch 503 Batch 0 Loss 0.0016\n",
            "Epoch 503 Loss 0.0010\n",
            "Time taken for 1 epoch 5.43498301506 sec\n",
            "\n",
            "Epoch 504 Batch 0 Loss 0.0017\n",
            "Epoch 504 Loss 0.0010\n",
            "Time taken for 1 epoch 5.82424497604 sec\n",
            "\n",
            "Epoch 505 Batch 0 Loss 0.0017\n",
            "Epoch 505 Loss 0.0010\n",
            "Time taken for 1 epoch 5.36802315712 sec\n",
            "\n",
            "Epoch 506 Batch 0 Loss 0.0016\n",
            "Epoch 506 Loss 0.0010\n",
            "Time taken for 1 epoch 5.89038491249 sec\n",
            "\n",
            "Epoch 507 Batch 0 Loss 0.0016\n",
            "Epoch 507 Loss 0.0010\n",
            "Time taken for 1 epoch 5.3762421608 sec\n",
            "\n",
            "Epoch 508 Batch 0 Loss 0.0016\n",
            "Epoch 508 Loss 0.0010\n",
            "Time taken for 1 epoch 5.8129119873 sec\n",
            "\n",
            "Epoch 509 Batch 0 Loss 0.0017\n",
            "Epoch 509 Loss 0.0010\n",
            "Time taken for 1 epoch 5.38655519485 sec\n",
            "\n",
            "Epoch 510 Batch 0 Loss 0.0017\n",
            "Epoch 510 Loss 0.0010\n",
            "Time taken for 1 epoch 5.94661402702 sec\n",
            "\n",
            "Epoch 511 Batch 0 Loss 0.0016\n",
            "Epoch 511 Loss 0.0010\n",
            "Time taken for 1 epoch 5.40819096565 sec\n",
            "\n",
            "Epoch 512 Batch 0 Loss 0.0016\n",
            "Epoch 512 Loss 0.0010\n",
            "Time taken for 1 epoch 5.85123085976 sec\n",
            "\n",
            "Epoch 513 Batch 0 Loss 0.0016\n",
            "Epoch 513 Loss 0.0010\n",
            "Time taken for 1 epoch 5.39247393608 sec\n",
            "\n",
            "Epoch 514 Batch 0 Loss 0.0016\n",
            "Epoch 514 Loss 0.0010\n",
            "Time taken for 1 epoch 5.81855106354 sec\n",
            "\n",
            "Epoch 515 Batch 0 Loss 0.0016\n",
            "Epoch 515 Loss 0.0010\n",
            "Time taken for 1 epoch 5.35588693619 sec\n",
            "\n",
            "Epoch 516 Batch 0 Loss 0.0016\n",
            "Epoch 516 Loss 0.0010\n",
            "Time taken for 1 epoch 5.88940811157 sec\n",
            "\n",
            "Epoch 517 Batch 0 Loss 0.0016\n",
            "Epoch 517 Loss 0.0010\n",
            "Time taken for 1 epoch 5.37427616119 sec\n",
            "\n",
            "Epoch 518 Batch 0 Loss 0.0016\n",
            "Epoch 518 Loss 0.0010\n",
            "Time taken for 1 epoch 5.90278220177 sec\n",
            "\n",
            "Epoch 519 Batch 0 Loss 0.0016\n",
            "Epoch 519 Loss 0.0010\n",
            "Time taken for 1 epoch 5.41210412979 sec\n",
            "\n",
            "Epoch 520 Batch 0 Loss 0.0016\n",
            "Epoch 520 Loss 0.0010\n",
            "Time taken for 1 epoch 5.87742877007 sec\n",
            "\n",
            "Epoch 521 Batch 0 Loss 0.0015\n",
            "Epoch 521 Loss 0.0009\n",
            "Time taken for 1 epoch 5.41428184509 sec\n",
            "\n",
            "Epoch 522 Batch 0 Loss 0.0018\n",
            "Epoch 522 Loss 0.0010\n",
            "Time taken for 1 epoch 5.89491701126 sec\n",
            "\n",
            "Epoch 523 Batch 0 Loss 0.0018\n",
            "Epoch 523 Loss 0.0010\n",
            "Time taken for 1 epoch 5.38204407692 sec\n",
            "\n",
            "Epoch 524 Batch 0 Loss 0.0023\n",
            "Epoch 524 Loss 0.0012\n",
            "Time taken for 1 epoch 5.89223980904 sec\n",
            "\n",
            "Epoch 525 Batch 0 Loss 0.0017\n",
            "Epoch 525 Loss 0.0010\n",
            "Time taken for 1 epoch 5.35745191574 sec\n",
            "\n",
            "Epoch 526 Batch 0 Loss 0.0020\n",
            "Epoch 526 Loss 0.0019\n",
            "Time taken for 1 epoch 5.88967895508 sec\n",
            "\n",
            "Epoch 527 Batch 0 Loss 0.0035\n",
            "Epoch 527 Loss 0.0021\n",
            "Time taken for 1 epoch 5.39962506294 sec\n",
            "\n",
            "Epoch 528 Batch 0 Loss 0.0134\n",
            "Epoch 528 Loss 0.0060\n",
            "Time taken for 1 epoch 5.89669299126 sec\n",
            "\n",
            "Epoch 529 Batch 0 Loss 0.0250\n",
            "Epoch 529 Loss 0.0183\n",
            "Time taken for 1 epoch 5.37378096581 sec\n",
            "\n",
            "Epoch 530 Batch 0 Loss 0.0370\n",
            "Epoch 530 Loss 0.0766\n",
            "Time taken for 1 epoch 5.86502885818 sec\n",
            "\n",
            "Epoch 531 Batch 0 Loss 0.1194\n",
            "Epoch 531 Loss 0.1661\n",
            "Time taken for 1 epoch 5.39230680466 sec\n",
            "\n",
            "Epoch 532 Batch 0 Loss 0.2640\n",
            "Epoch 532 Loss 0.3536\n",
            "Time taken for 1 epoch 5.90352988243 sec\n",
            "\n",
            "Epoch 533 Batch 0 Loss 0.5315\n",
            "Epoch 533 Loss 0.3779\n",
            "Time taken for 1 epoch 5.35037183762 sec\n",
            "\n",
            "Epoch 534 Batch 0 Loss 0.4209\n",
            "Epoch 534 Loss 0.3074\n",
            "Time taken for 1 epoch 5.87105298042 sec\n",
            "\n",
            "Epoch 535 Batch 0 Loss 0.2402\n",
            "Epoch 535 Loss 0.2323\n",
            "Time taken for 1 epoch 5.32706999779 sec\n",
            "\n",
            "Epoch 536 Batch 0 Loss 0.1804\n",
            "Epoch 536 Loss 0.1578\n",
            "Time taken for 1 epoch 5.90246391296 sec\n",
            "\n",
            "Epoch 537 Batch 0 Loss 0.1331\n",
            "Epoch 537 Loss 0.1201\n",
            "Time taken for 1 epoch 5.36707782745 sec\n",
            "\n",
            "Epoch 538 Batch 0 Loss 0.0938\n",
            "Epoch 538 Loss 0.0823\n",
            "Time taken for 1 epoch 5.90591001511 sec\n",
            "\n",
            "Epoch 539 Batch 0 Loss 0.0670\n",
            "Epoch 539 Loss 0.0613\n",
            "Time taken for 1 epoch 5.40544009209 sec\n",
            "\n",
            "Epoch 540 Batch 0 Loss 0.0555\n",
            "Epoch 540 Loss 0.0476\n",
            "Time taken for 1 epoch 5.91932821274 sec\n",
            "\n",
            "Epoch 541 Batch 0 Loss 0.0446\n",
            "Epoch 541 Loss 0.0386\n",
            "Time taken for 1 epoch 5.36671996117 sec\n",
            "\n",
            "Epoch 542 Batch 0 Loss 0.0343\n",
            "Epoch 542 Loss 0.0300\n",
            "Time taken for 1 epoch 5.81798100471 sec\n",
            "\n",
            "Epoch 543 Batch 0 Loss 0.0268\n",
            "Epoch 543 Loss 0.0247\n",
            "Time taken for 1 epoch 5.39587807655 sec\n",
            "\n",
            "Epoch 544 Batch 0 Loss 0.0214\n",
            "Epoch 544 Loss 0.0205\n",
            "Time taken for 1 epoch 5.91900587082 sec\n",
            "\n",
            "Epoch 545 Batch 0 Loss 0.0176\n",
            "Epoch 545 Loss 0.0168\n",
            "Time taken for 1 epoch 5.36770200729 sec\n",
            "\n",
            "Epoch 546 Batch 0 Loss 0.0155\n",
            "Epoch 546 Loss 0.0144\n",
            "Time taken for 1 epoch 5.92914891243 sec\n",
            "\n",
            "Epoch 547 Batch 0 Loss 0.0136\n",
            "Epoch 547 Loss 0.0124\n",
            "Time taken for 1 epoch 5.37880110741 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSfG9XHkx05n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_inp,\n",
        "                                                           padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                             dec_hidden,\n",
        "                                                             enc_out)\n",
        "\n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-mM6J45x3V5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 14}\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qqa_Agqx5lL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))\n",
        "\n",
        "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rucHhMiU0_bN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate('Então Deus disse:— Que haja luz! E a luz começou a existir. 4Deus viu que a luz era boa e a separou da escuridão.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fP8sc-RmuVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_test_dataset(path, train_size, test_size):\n",
        "  \n",
        "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "   \n",
        "    sentences = []\n",
        "    original = []\n",
        "    for line in lines[train_size:train_size+test_size]:\n",
        "     \n",
        "      test = line.split('\\t')\n",
        "      original.append(test[0]) \n",
        "      sentences.append(test[1]) \n",
        "      \n",
        "    return original, sentences\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLM4imtCW2jq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "def bleu_accuracy(original, sentences):\n",
        "  \n",
        "  translation = []\n",
        "  for sentence in sentences:\n",
        "\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "    result = result.split()\n",
        "    result.pop()\n",
        "    translation.append(result)\n",
        "\n",
        " \n",
        "  \n",
        "  references = []\n",
        "  \n",
        "  for ref in original:\n",
        "    references.append(ref.split())\n",
        "    \n",
        "  \n",
        "  scores = []\n",
        "  \n",
        " \n",
        "  smoth = SmoothingFunction()\n",
        "  for hyp, ref, weight in zip(translation, references, attention_plot):\n",
        "    \n",
        "    score = sentence_bleu([ref], hyp )\n",
        "    \n",
        "    scores.append(score)\n",
        " \n",
        "  return np.mean(score)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlLPHxBLK-FA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def saving_result(treino, teste, acc):\n",
        "  \n",
        "  file = open(\"test_result.txt\", 'a')\n",
        "\n",
        "  file.write('Train ' + str(treino) + '\\tTest ' + str(teste) + '\\tAcc ' + str(acc) + '\\n')\n",
        "\n",
        "  file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZGNO7QrD2cF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "treino = len(input_tensor_train)\n",
        "teste = len(input_tensor_val)\n",
        "\n",
        "original, sentences = load_test_dataset('gu-pt_v1.txt', treino, teste)\n",
        "bleu = bleu_accuracy(original, sentences)\n",
        "print(bleu)\n",
        "\n",
        "\n",
        "saving_result(treino, teste, bleu)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}